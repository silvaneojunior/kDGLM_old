---
title: "Caso Gamma"
author: "Silvaneo Viera dos Santos Junior"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
subtitle: Relatório
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 8,
	fig.width = 12,
	message = FALSE,
	warning = FALSE,
	comment = ''
)
library(tidyverse)
library(lamW)
library(latex2exp)
devtools::load_all()
```

## Introdução

Neste relatório apresentaremos os resultados das análises feitas sobre o GDLM k-paramétrico para o caso Gamma com parâmetro de forma e média desconhecidos. Para esta análise, vamos assumir o seguinte modelo observacional:

$$
X|\phi,\mu \sim \mathcal{G}\left(\phi,\frac{\phi}{\mu}\right),
$$
onde $\mathcal{G}$ representa a distribuição Gamma. Por conveniência, usaremos a parametrização com $\alpha=\phi$ e $\beta=\frac{\phi}{\mu}$ de modo que: 

$$
X|\alpha,\beta \sim \mathcal{G}\left(\alpha,\beta\right),
$$
sendo que transitar de uma parametrização para a outra é trivial e a resolução dos sistemas de compatibilização é a mesma.

Para os parâmetros $\alpha$ e $\beta$, temos que a priori conjugada é tal que:

$$
\pi(\alpha,\beta) \propto \exp\left\{n_0\alpha \ln(\beta)-k_0\ln(\Gamma(\alpha))+\theta_0\alpha-\tau_0\beta\right\},
$$
onde $n_0$, $k_0$, $\theta_0$ e $\tau_0$ são os parâmetros da distribuição e $\Gamma$ é a função *Gamma*. Quando um par de variáveis aleatórias $X,Y$ tiver a densidade descrita acima, diremos que $X,Y \sim \Pi(n_0,k_0,\theta_0,\tau_0)$, sendo que $n_0, k_0, \tau_0>0$. No caso especial onde $n_0=k_0$, diremos que $X,Y \sim \Pi(n_0,\theta_0,\tau_0)$.

Ao obter uma amostra de tamanho $m$ do modelo observacional, a obtenção dos parâmetros da posteriori ($n_m$, $k_m$, $\theta_m$ e $\tau_m$) pode ser feita a partir das equações a seguir:

$$
\begin{aligned}
n_m=&n_0+m\\
k_m=&k_0+m\\
\theta_m=&\theta_0+\sum_{i=1}^m\ln(x_i)\\
\tau_m=&\tau_0+\sum_{i=1}^m x_i.
\end{aligned}
$$

Por último, esta distribuição pertence à família exponencial e o vetor de estatísticas suficientes associado a esta distribuição é:

$$
H_p=(\alpha,\beta,\alpha \ln(\beta),\ln(\Gamma(\alpha)))'
$$

Para utilizar o método proposto no artigo k-paramétrico é necessário obter $\mathbb{E}_p[{H_p}]$ e $\mathbb{E}_q[{H_p}]$, onde $E_p$ é o valor esperado calculado com $\alpha$ e $\beta$ tendo a distribuição conjugada $p$ e $E_q$ é o valor esperado calculado com $\alpha$ e $\beta$ tendo distribuição log-Normal.

Na próxima sessão discutiremos algumas propriedades da distribuição $\Pi$, pois diversos problemas encontrados tem sua origem nas características de $\Pi$.

Na sessão subsequente abordaremos os resultados da tentativa de se calcular $\mathbb{E}_p[{H_p}]$ usando aproximações de Laplace (Tierney e Kadane, 1995). Infelizmente, não conseguimos obter um ajuste funcional com esta abordagem devido a problemas na solução do sistema $\mathbb{E}_p[{H_p}]=\mathbb{E}_q[{H_p}]$.

Na última sessão apresentamos uma proposta que permite obter uma expressão analítica aproximada para $\mathbb{E}_p[{H_p}]$. Com isso, conseguimos resolver o sistema (ainda usando Newton-Raphson, mas sem problemas numéricos) e fazer o ajuste do modelo. Ainda assim, o ajuste deixa a desejar. Mais investigações estão sendo feitas para tentar identificar o problema.

## Propriedades da distribuição $\Pi$

Primeiro, observemos que, se $\alpha, \beta \sim \Pi\left(n_0,k_0,\theta_0,\tau_0\right)$, então:

$$
\begin{aligned}
f(\beta|\alpha)&\propto \pi(\alpha,\beta)\propto\exp\left\{n_0\alpha \ln(\beta)-k_0\ln(\Gamma(\alpha))+\theta_0\alpha-\tau_0\beta\right\}\\
& \propto \exp\left\{n_0\alpha \ln(\beta)-\tau_0\beta\right\}=\beta^{n_0\alpha}e^{-\tau_0\beta},
\end{aligned}
$$
ou seja $\beta|\alpha \sim \mathcal{G}(n_0 \alpha+1,\tau_0)$.

Usando a distribuição condicional de $\beta$ podemos reescrever $\mathbb{E}_p[H_p]=\mathbb{E}_p[\mathbb{E}_p[H_p|\alpha]]$, de onde obtemos:

$$
\begin{aligned}
\mathbb{E}_p[\beta]&=\mathbb{E}_p[\mathbb{E}_p[\beta|\alpha]]=\mathbb{E}_p\left[\frac{n_0 \alpha +1}{\tau_0}\right]=\frac{n_0 \mathbb{E}_p\left[\alpha\right] +1}{\tau_0}\\
\mathbb{E}_p[\alpha\ln(\beta)]&=\mathbb{E}_p[\alpha\mathbb{E}_p[\ln(\beta)|\alpha]]=\mathbb{E}_p\left[\alpha(\psi(n_0 \alpha +1 )-\ln(\tau_0))\right]\\
&=\mathbb{E}_p\left[\alpha\psi(n_0 \alpha +1 )\right]-\ln(\tau_0)\mathbb{E}_p\left[\alpha\right].
 \end{aligned}
$$
Usando que $\mathbb{E}_p\left[\alpha\right]=\mathbb{E}_q\left[\alpha\right]$ ($\mathbb{E}_q\left[H_p\right]$ é suposto conhecido), temos que:

$$
n_0=\frac{\mathbb{E}_q\left[\beta\right]\tau_0-1}{\mathbb{E}_q\left[\alpha\right]}
$$

Com as equações acimas, conseguimos escrever $\mathbb{E}_p[H_p]$ como valores esperados que dependem apenas da distribuição marginal de $\alpha$, o que pode ser útil para simplicar algumas integrais e possibilitar a resolução numérica com métodos determinísticos. Vale observar que a distribuição marginal de $\alpha$ é tal que:

$$
\begin{aligned}
f(\alpha)&\propto \int_0^{+\infty}\pi(\alpha,\beta) d\beta\\
&\propto \int_0^{+\infty}\exp\left\{n_0\alpha \ln(\beta)-k_0\ln(\Gamma(\alpha))+\theta_0\alpha-\tau_0\beta\right\}d\beta\\
&= \exp\{-k_0\ln(\Gamma(\alpha))+\theta_0\alpha\}\int_0^{+\infty}\exp\left\{n_0\alpha \ln(\beta)-\tau_0\beta\right\}d\beta\\
&= \exp\{-k_0\ln(\Gamma(\alpha))+\theta_0\alpha\}\int_0^{+\infty}\beta^{n_0\alpha+1-1}e^{-\tau_0\beta}d\\\
&= \exp\{-k_0\ln(\Gamma(\alpha))+\theta_0\alpha\}\frac{\Gamma(n_0\alpha+1)}{\tau_0^{n_0\alpha+1}}\\
&= \exp\{-k_0\ln(\Gamma(\alpha))+\theta_0\alpha+\ln(\Gamma(n_0\alpha+1))-(n_0\alpha+1)\ln(\tau_0)\}\\
&\propto \exp\{\ln(\Gamma(n_0\alpha+1))-k_0\ln(\Gamma(\alpha))+(\theta_0-n_0\ln(\tau_0))\alpha\}\\
&= \frac{\Gamma(n_0\alpha+1)}{\Gamma(\alpha)^{k_0}}\exp\left\{(\theta_0-n_0\ln(\tau_0))\alpha\right\}.
 \end{aligned}
$$

Usando a densidade acima e aproveitando a escrita de $\mathbb{E}_p[H_p]$ como uma valor esperado em $\alpha$, podemos obter $\mathbb{E}_p[H_p]$ usando quadratura Gaussiana e dispensando o uso da aproximação de Laplace.

Como discutido em outros reuniões, um pre-requisito para o uso do Teorema da Projeção é que o valor esperado de $H_p$ exista e seja finito. Caso esta condição não seja satisfeita, não podemos fazer a compatibilização das prioris normal e conjugada. Assim, devemos encontrar as condições para as quais $\Pi$ é própria (i.e., a constante de normalização de $\pi$ é finita) e o valor esperado de $H_p$ é finito. Para facilitar esta análise, podemos fazer o estudo da distribuição marginal de $\alpha$, pois se $\Pi$ é própria, $\alpha$ também será, ademais, podemos avaliar o valor esperado de $H_p$ olhando apenas para a distribuição de $\alpha$.

Adiante, vamos exibir a densidade não normalizada de $\alpha$ para vários valores de $k_0$, $n_0$, $\theta_0$ e $\tau_0$, porém, como são muitas combinações de parâmetros, a análise acaba se tornando exaustiva, por isso, antes de apresentar os gráficos, vamos resumir as conclussões:

- Se $n_0$ e $k_0$ são grandes em comparação a $\tau_0$ e $\theta_0$ a densidade de $\alpha$ se torna crescente em $\alpha$ a partir de algum valor de $\alpha$ (como conseguência, a distribuição marginal de $\alpha$ não é própria). Observe que, para uma amostra grande, temos que $n_m\approx k_m \approx m$,$\tau_m \approx n_m\sum_{i=1}^{m}x_i/m$,$\theta_m \approx k_m\sum_{i=1}^{m}\ln(x_i)/m$, assim, a observação feita neste item equivale a dizer que a média dos $x_i$'s e dos $\ln(x_i)$'s não pode ser demasiadamente pequena.

- Se $n_0\ge k_0$ a densidadde de $\alpha$ se torna crescente em $\alpha$ para grande parte dos possíveis valores de $\tau_0$ e $\theta_0$.

Para garantir que a distribuição marginal de $\alpha$ seja própria, precisamos que $n_0$ seja significativamente maior que $k_0$ (o quão menor vai depender da escala de $k_0$) e/ou que $\tau_0$ e $\theta_0$ estejam compatíveis com a escala de $k_0$ e $n_0$. Em geral, essa informação será relevante apenas para inicialização do Newton-Raphson, sendo necessária uma escolha que evite que o algoritmo passe por "regiões ruins".

A seguir, apresentamos a densidade marginal não normalizada de $\alpha$ para $n_0=k_0=1$ e diversos valores de $\tau_0$ e $\theta_0$:

```{r}
calc_alpha_marg=function(n0,k0,theta0,tau0,x=seq(0,100,l=1000)){
  f_densi=function(x){-k0*lgamma(x)+theta0*x+lgamma(n0*x+1)-(n0*x+1)*log(tau0)}
  # c_val=integrate(f_densi,0,Inf)
  f_densi(x)
}
x=seq(0,100,l=1000)

plot.data=data.frame()
for(n0 in c(1)){
  for(k0 in c(1)){
    for(theta0 in c(-2:2)){
      for(tau0 in exp(c(-2:2))){
        f=calc_alpha_marg(n0=n0,k0=k0,theta0=theta0,tau0=tau0,seq(0,100,l=1000))
        plot.data=rbind(plot.data,
                        data.frame(x=x,
                                   f=exp(f-max(f)),
                                   n0=n0,
                                   k0=k0,
                                   theta0=theta0,
                                   tau0=paste0('e^',log(tau0))))
      }
    }
  }
}
plot.data$tau0=factor(plot.data$tau0,levels=paste0('e^',-2:2))
```


```{r}
ggplot(plot.data)+
  geom_line(aes(x=x,y=f))+
  scale_x_continuous('',sec.axis=sec_axis(trans=~., name="$\\theta_0$" %>% TeX))+
  scale_y_continuous("$\\tau_0$" %>% TeX)+
  theme_bw()+
  theme(axis.text.x = element_blank(),axis.text.y = element_blank())+
  facet_grid(tau0~theta0,scales='free',switch='y')+
  labs(title='$n_0=1$ e $k_0=1$'%>% TeX)
```

Veja que, se $\tau_0=e^{\theta_0}$, a densidade de $\alpha$ é simplemente uma reta crescente em $\alpha$. Se $\tau_0>e^{\theta_0}$ a densidade de $\alpha$ é própria e se $\tau_0 \le e^{\theta_0}$ a desidade de $\alpha$ é crescente em $\alpha$ (a partir de algum valor), logo a distribuição de $\alpha$ não é própria.

```{r}
calc_alpha_marg=function(n0,k0,theta0,tau0,x=seq(0,100,l=1000)){
  f_densi=function(x){-k0*lgamma(x)+theta0*x+lgamma(n0*x+1)-(n0*x+1)*log(tau0)}
  # c_val=integrate(f_densi,0,Inf)
  f_densi(x)
}
x=seq(0,100,l=1000)

plot.data=data.frame()
for(n0 in c(2)){
  for(k0 in c(2)){
    for(theta0 in c(-2:2)){
      for(tau0 in exp(c(-2:2))){
        f=calc_alpha_marg(n0=n0,k0=k0,theta0=n0*theta0,tau0=k0*tau0,seq(0,100,l=1000))
        plot.data=rbind(plot.data,
                        data.frame(x=x,
                                   f=exp(f-max(f)),
                                   n0=n0,
                                   k0=k0,
                                   theta0=paste0('k0*',theta0),
                                   tau0=paste0('n0*e^',log(tau0))))
      }
    }
  }
}
plot.data$theta0=factor(plot.data$theta0,levels=unique(plot.data$theta0))
plot.data$tau0=factor(plot.data$tau0,levels=unique(plot.data$tau0))
```


```{r}
ggplot(plot.data)+
  geom_line(aes(x=x,y=f))+
  scale_x_continuous('',sec.axis=sec_axis(trans=~., name="$\\theta_0$" %>% TeX))+
  scale_y_continuous("$\\tau_0$" %>% TeX)+
  theme_bw()+
  theme(axis.text.x = element_blank(),axis.text.y = element_blank())+
  facet_grid(tau0~theta0,scales='free',switch='y')+
  labs(title='$n_0=2$ e $k_0=2$'%>% TeX)
```

Com $n_0=k_0=2$ temos um resultado parecido com o anterior, porém, o "ponto de corte" para tornar a densidade imprópria muda. De modo geral, quando $n_0=k_0=m$, observamos que o ponto de corte é $\ln\left(\frac{\tau_0}{m}\right)> \frac{\theta_0}{m}$. Intuitivamente, podemos entender a razão para este ponto de corte da seguinte forma: Se temos uma amostra de tamanho $m$ com $m$ muito grande, então $\tau_m\approx m\sum \frac{x_i}{m}$ e $\theta_m\approx m\sum \frac{\ln(x_i)}{m}$, então teríamos que $\ln\left(\frac{\tau_m}{m}\right)> \frac{\theta_m}{m}$, pois a função logarítmo é côncava, portanto $\ln\left(\frac{\tau_m}{m}\right) \approx \ln\left(\sum\frac{x_i}{m}\right)> \sum \frac{\ln(x_i)}{m}\approx \frac{\theta_m}{m}$. Ou seja, é "artificial" para uma $\Pi$ que a condição $\ln\left(\frac{\tau_0}{m}\right)> \frac{\theta_0}{m}$ não seja válida, pois dados reais nunca produziram parâmetros sem essa propriedade.


A partir das análises feitas não conseguimos encontrar uma regra que garanta que a densidade de $\alpha$ seja própria a menos que tomemos $n_0=k_0$, porém, ainda nesse caso, a restrição encontrada é inconveniente, pois a restrição $\ln\left(\frac{\tau_0}{m}\right)> \frac{\theta_0}{m}$ induz um espaço parametrico onde não há garantias de que exista um elemento que minimize a divergência KL. No geral, tivemos muitos problemas em encontrar o mínimo, pois o algoritmo frequentemente saí do conjunto válido de parâmetros. Em diversas ocasiões conseguimos encontrar um valor inicial para os parâmetros de modo que o algoritmo de Newton-Raphson convirja de forma adequada, mas não conseguimos estabelecer um critério geral que garanta que sempre poderemos resolver o sistema. Uma forma de mitigar esse problema seria através da simplificação dos sistemas (se possível), pois isso facilitaria a busca dos parâmetros.

## Método de Laplace

Suponhamos que queremos calcular $\mathbb{E}[g(x)]$ com $x$ tendo densidade proporcional a $f^*$ e $g$ sendo uma função positiva, então podemos escrever:

$$
\mathbb{E}[g(x)]=\frac{\int_\mathbb{R}g(x)f^*(x)dx}{\int_\mathbb{R}f^*(x)dx}.
$$

Se considerarmos que:

$$
\begin{aligned}
g(x)f^*(x)&\approx \exp\left\{L_1(x^*_1)-\frac{(x-x_1^*)^2}{2v_1}\right\},\\
f^*(x)&\approx \exp\left\{L_2(x^*_2)-\frac{(x-x_2^*)^2}{2v_2}\right\},
\end{aligned}
$$
onde $L_1(x)=\ln(g(x)f^*(x))$, $L_2(x)=\ln(f^*(x))$, $x_i^*$ é o argumento que maximiza $L_i$ e $v_i=-L_i''^{-1}(x_i^*)$. Usando a aproximação acima, obtemos:

$$
\mathbb{E}[g(x)]\approx \left(\frac{v_1}{v_2}\right)^{\frac{1}{2}} \exp\left\{L_1(x^*_1)-L_2(x^*_2)\right\}
$$

A abordagem acima pode ser facilmente generalizada para o caso onde $x$ é um vetor.

A ideia proposta pelo Migon é utilizar o método descrito para calcular $\mathbb{E}_p[H_p]$, porém encontramos um problema ao tentar por em prática esta proposta: Para calcular $\mathbb{E}_p[H_p]$ pelo método de Laplace precisamos conhecer os parâmetros da distribuição conjugada, mas desejamos calcular $\mathbb{E}[g(x)]$ justamente para encontrar os parâmetros da distribuição conjugada.

Para apresentar o problema de forma clara, vamos descrever o algoritimo de Newton-Raphson para a solução do sistema $\mathbb{E}_p[H_p]=\mathbb{E}_q[H_p]$:

- Passo 0: Suponha que conhecemos $\mathbb{E}_q[H_p]$ e seja $\phi=\mathbb{E}_{p}[H_p]$ com $p=\Pi(n,k,\tau,\theta)$.
- Passo 1: Inicializamos escolhendo $n, k, \tau$ e $\theta$ como valores válidos.
- Passo 2: Calculamos $\phi$ e $\nabla \phi$ (a matriz de derivadas parciais de $\phi$ com relação aos parâmetro de $\Pi$). Caso não exista a forma analítica para $\nabla \phi$, devemos avaliar $\phi$ 4 vezes (além da avaliação inicial) para calcular numericamente as derivadas de $\phi$.
- Passo 3: Atualizamos $n, k, \tau$ e $\theta$ segundo o algoritmo de Newton-Raphson.
- Passo 4: Se $\phi$ é suficientemente próximo de $\mathbb{E}_q[H_p]$ encerramos o algoritmo, do contrário voltamos ao passo 2. 
 
Usando o método de Laplace para obter $\phi$, digamos, para encontrar $\mathbb{E}_p[\alpha]$, então temos que $g(\alpha,\beta)=\alpha$ e devemos encontrar $\alpha_1^*$, $\beta_1^*$, $\alpha_2^*$ e $\beta_2^*$ que maximizam $L_1$ e $L_2$, respectivamente. Infelizmente, não é possível obter uma forma analítica fechada para $\alpha_1^*$ e $\beta_1^*$, de modo que seria necessário usar o método de Newton-Raphson para encontrar esses valores, porém isto está ocorrendo dentro de **uma** avaliação de **uma** das componentes de $\phi$ para **uma** iteração do método de Newton-Raphson, ou seja, seria necessário usar o método de Newton-Raphson 20 vezes **para cada iteração** do método de Newton-Raphson principal (temos de calcular $\phi$ 5 vezes a cada iteração e em cada cálculo precisamos usar Newton-Raphson 4 vezes). Por conta disso, é inviável usar a abordagem acima para realizar a compatibilização das prioris. Se fosse possível obter $\alpha_1^*$, $\beta_1^*$ de forma analítica para todos os parâmetro, não haveria problema, porém, não sendo este o caso, se torna inviável a resolução do sistema.

Como alternativa ao método descrito acima, podemos usar o seguinte fato, se $\Pi(n,k,\theta,\tau)$ pertence à família exponencial, então:

$$\mathbb{E}_p[H_p]=\nabla A(n,k,\theta,\tau),$$
onde $A(n,k,\theta,\tau)$ é o logarítimo da constante de normalização de $\Pi(n,k,\theta,\tau)$, isto é:

$$
\exp\{A(n,k,\theta,\tau)\}=\left(\int_0^{+\infty}\int_0^{+\infty}\exp\left\{n\alpha \ln(\beta)-k\ln(\Gamma(\alpha))+\theta\alpha-\tau\beta\right\}d\beta d\alpha\right)^{-1}.
$$

Pelo método de Laplace, temos que:

$$
\exp\{A(n,k,\theta,\tau)\}\approx \sqrt{2\pi v_2} \exp\left\{L_2(\alpha^*_2,\beta_2^*)\right\},
$$
sendo que, especificamente para este caso, por sorte, há forma analítica aproximada para $\alpha^*_2$,$\beta_2^*$. De fato, veja que $L_2(\alpha,\beta)=n\alpha \ln(\beta)-k\ln(\Gamma(\alpha))+\theta\alpha-\tau\beta$, daí:

$$
\begin{aligned}
\frac{\partial}{\partial \alpha}L_2(\alpha,\beta)&=n \ln(\beta)-k\psi(\alpha)+\theta\\
\frac{\partial}{\partial \beta}L_2(\alpha,\beta)&=n\frac{\alpha}{\beta}-\tau,
\end{aligned}
$$
onde $\psi$ é a função *digamma*.

Da segunda equação obtemos que:

$$
\begin{aligned}
\frac{\partial}{\partial \beta}L_2(\alpha^*,\beta^*)=0 \iff n\frac{\alpha^*}{\beta^*}=\tau \iff \beta^*=\frac{n}{\tau}\alpha^*
\end{aligned}
$$

Substitituindo o valor de $\beta^*$ na primeira equação e usando uma aproximação de primeira ordem para a função digamma obtemos que:

$$
\begin{aligned}
\frac{\partial}{\partial \alpha}L_2(\alpha^*,\beta^*)&=n \ln\left(\frac{n}{\tau}\right)+n \ln(\alpha^*)-k\psi(\alpha^*)+\theta,\\
&\approx n \ln\left(\frac{n}{\tau}\right)+n \ln(\alpha^*)-k\ln(\alpha^*)+\frac{k}{2\alpha^*}+\theta,
\end{aligned}
$$

Usando o Wolfram, encontramos que $\frac{\partial}{\partial \alpha}L_2(\alpha^*,\beta^*)=0$ se, e somente se:

$$
\alpha^*=\frac{k}{2(k-n)W\left(\frac{k(2^{1-\frac{n}{k}}e^{\theta/k}\frac{n}{\tau}^{n/k})^{-k/(k-n)}}{k-n}\right)},
$$
onde $W$ é a função $W$ de Lambert.

Veja que a solução proposta não está bem definida no caso $n=k$, ademais, como discutido anteriormente, a aproximação de primeira ordem para a função *digamma* deixa muito a desejar. Contudo, no caso onde $n=k$ podemos usar uma aproximação de segunda ordem para a função *digamma*, pois, neste caso, obtemos:

$$
\frac{\partial}{\partial \alpha}L_2(\alpha^*,\beta^*)
\approx n \ln\left(\frac{n}{\tau}\right)+n \ln(\alpha^*)-k\ln(\alpha^*)+\frac{k}{2\alpha^*}+\frac{k}{12\alpha^{*2}}+\theta=n \ln\left(\frac{n}{\tau}\right)+\frac{n}{2\alpha^*}+\frac{k}{12\alpha^{*2}}+\theta,
$$
daí:

$$
\frac{\partial}{\partial \alpha}L_2(\alpha^*,\beta^*)=0 \iff \alpha^*=\frac{1}{3+\sqrt{9+12\left(\ln\left(\frac{\tau}{n}\right)-\frac{\theta}{n}\right)}}.
$$

Observe que, na solução acima, se $\ln\left(\frac{\tau}{n}\right)<\frac{\theta}{n}-\frac{3}{4}$, então $\alpha^*$ não está bem definido, o que é algo problemático. Dito isso, como visto anteriorimente, já é necessário que $\ln\left(\frac{\tau}{n}\right)>\frac{\theta}{n}$, a restrição $\ln\left(\frac{\tau}{n}\right)>\frac{\theta}{n}-\frac{3}{4}$ já está sendo satisfeita.

Vale destacar que, se $\Pi$ é a posteriori depois de se observar uma amostra de tamanho $m$ do modelo observacional ($m>>0$), então $\ln \left(\frac{\tau}{n}\right) \approx \ln\left(\frac{1}{m}\sum x_i\right)$ e $\theta/n \approx \frac{1}{m}\sum \ln(x_i)$, daí, como a função log é concava, vale que $\ln\left(\frac{\tau}{n}\right)>\frac{\theta}{n}$, ou seja, $\alpha^*$ está bem definido, logo, após observar uma quantidade razoável de dados, estaremos livres do risco de que $\alpha^*$ não existir.

Uma vez obtido $\alpha^*$ e $\beta^*$, podemos obter uma aproximação para $A(n,k,\theta,\tau)$ e então obter $\mathbb{E}_p[H_p]=\nabla A(n,k,\theta,\tau)$.

Como mencionado anteriormente, também podemos obter $\mathbb{E}_p[H_p]$ por integração numérica e obter um resultado semelhante. A princípio, não seria viável obter $\mathbb{E}_p[H_p]$ por integração numérica, pois a integral que devemos calcular é dupla, inviabilizando os métodos determinísticos que conheço (eles funcionam, mas ficam com o custo computacional irrazoável), ademais, não podemos usar integração por Monte Carlo, pois, para o método de Newton-Raphson, devemos calcular as derivadas de $\mathbb{E}_p[H_p]$ e como não conhecemos a forma analítica de $\mathbb{E}_p[H_p]$, devemos recorrer a diferenciação numérica, porém o erro de Monte Carlo impede que isso seja feito (o ruído aleatório intrínsico ao método de Monte Carlo "ofusca" o valor das derivadas, sendo necessário usar amostras irrazoavelmente grandes para contornar esse problema).

Dito isso, durante a construção deste relatório, observamos que $\mathbb{E}_p[H_p]$ pode ser escrito como uma integral que depende apenas de $\alpha$, o que viabiliza o uso de métodos numérico determinísticos para calcular $\mathbb{E}_p[H_p]$ (especificamente, usamos Quadratura Gaussiana, que é o método *default* da função *integrate* do *R*), usando essa abordagem obtemos um resultado parecido com o resultado usando a aproximação de Laplace, porém, acredito que o uso de Quadratura Gaussiana seja mais adequado, pois evita o uso de aproximações, gerando assim valores que são numericamente iguais ao verdadeiro.

Vale destacar que, apesar da abordagem usando o método de Laplace não ser a que recomendo, acredito que o desenvolvimento dessa abordagem foi útil para dar alguns *insights* que não obteríamos se tivéssemos feito uso de integração numérica desde o início.

Isso concluí a análise sobre o uso da aproximação de Laplace para calcular $\mathbb{E}_p[H_p]$. Resolvido esta questão, resta apenas usar o algoritmo de Newton-Raphson para resolver o sistema $\mathbb{E}_p[H_p]=\mathbb{E}_q[H_p]$.

Como mencionado anteriormente, há certas escolhas de parâmetros para a distribuição conjugada para as quais $\Pi$ não é própria, sendo que não conseguimos uma forma de garantir que o algoritmo de Newton-Raphson não passe por parâmetros inadequados. Não bastasse isso, também não conseguimos garantir que o sistema sequer tenha solução.

Tentando aplicar o a metodologia em dados simulados, independente da inicialização do modelo, sempre há alguma iteração onde o algoritmo de Newton-Raphson tem problemas. Ademais também reparamos que há uma distorção considerável dos dados após a compatibilização das prioris, de modo que tivemos problemas mesmo nas ocasiões em que o algoritmo de Newton-Raphson de fato convergiu.

Por último, apresentarei na próxima sessão uma abordagem alternativa que permite simplificar a resolução do sistema e obter forma analítica aproximada para $\mathbb{E}_p[H_p]$.

## Proposta alternativa: Aproximação da distribuição marginal de $\alpha$

Lembremos que, se $\alpha,\beta \sim \Pi(n,k,\tau,\theta)$, então $\beta|\alpha \sim \mathcal{G}(n\alpha+1,\tau)$ e:

$$
f(\alpha)\propto \frac{\Gamma(n\alpha+1)}{\Gamma(\alpha)^{k}}\exp\left\{(\theta-n\ln(\tau))\alpha\right\}.
$$

Ademais, a fórmula de Stirling nos diz que:

$$
\Gamma(x+1) \approx \sqrt{2\pi x}\left(\frac{x}{e}\right)^x.
$$
Usando essa aproximação na densidade de $\alpha$ obtemos que:

$$
\begin{aligned}
f(\alpha)&\propto \frac{\Gamma(n\alpha+1)}{\Gamma(\alpha)^{k}}\exp\left\{(\theta-n\ln(\tau))\alpha\right\}\\
&= \frac{\Gamma(n\alpha+1)}{\alpha^{-k}\Gamma(\alpha+1)^{k}}\exp\left\{(\theta-n\ln(\tau))\alpha\right\}\\
&\approx\frac{\sqrt{2\pi n}}{\sqrt{2\pi}^k} \frac{\alpha^{\frac{1}{2}}n^{n\alpha}\alpha^{n\alpha}e^{-n\alpha}}{\alpha^{-k}\alpha^{\frac{k}{2}}\alpha^{k\alpha}e^{-k\alpha}}\exp\left\{(\theta-n\ln(\tau))\alpha\right\}\\
&= \frac{\sqrt{n}}{(2\pi)^{\frac{k-1}{2}}}\alpha^{\frac{1}{2}+\frac{k}{2}}\alpha^{(n-k)\alpha}e^{-(n-k)\alpha}\exp\left\{(\theta-n\ln(\tau/n))\alpha\right\}.
\end{aligned}
$$

Até aqui a equação acima não é particularmente útil, porém, se assumirmos $n=k$, obtemos:

$$
\begin{aligned}
f(\alpha)&\propto \alpha^{\frac{1}{2}+\frac{n}{2}}\exp\left\{(\theta-n\ln(\tau/n))\alpha\right\}.
\end{aligned}
$$

Ao olhar com carinho, podemos reparar que $\alpha$ tem distribuição aproximada $\mathcal{G}\left(\frac{n+3}{2},n\ln(\tau/n)-\theta\right)$ (e esta é uma boa aproximação). Com essa informação, temos que:

$$
\mathbb{E}_p[\alpha] \approx \frac{n+3}{2(n\ln(\tau/n)-\theta)}
$$

O que pode ser usado para resolver mais uma das equações do sistema $\mathbb{E}_p[H_p]=\mathbb{E}_q[H_p]$ sem recorrer a métodos numéricos.

Para obter solução analítica para todo o sistema resta apenas calcular um valor esperado, uma vez que, ao supor $n=k$ reduzimos o sistema para $3$ equações, das quais temos solução analítica para duas. Se conseguirmos resolver a última de forma analítica, podemos conseguir uma forma garantida de fazer a compatibilização das prioris sem depender de métodos iterativos.

O valor esperado que resta calcular é $\mathbb{E}_p[\alpha\ln(\beta)-\ln(\Gamma(\alpha))]=\mathbb{E}_p\left[\alpha\psi(n \alpha +1 )-\ln(\tau)\alpha-\ln(\Gamma(\alpha))\right]$ (note que, ao supor $n=k$, $H_p$ se tornou outro vetor). Usando que:

$$
\begin{aligned}
\psi(n \alpha +1 ) &\approx \psi(n \alpha) \approx \ln(n\alpha)-\frac{1}{2n\alpha}-\frac{1}{12n^2\alpha^2}\\
\ln(\Gamma(\alpha)) &\approx \alpha\ln(\alpha)-\frac{1}{2}\ln(\alpha)+\frac{1}{12\alpha},
\end{aligned}
$$
podemos obter a seguinte aproximação:

$$
\begin{aligned}
\alpha\psi(n \alpha +1 )-\ln(\tau)\alpha-\ln(\Gamma(\alpha)) &\approx \alpha\ln(n\alpha)-\frac{1}{2n}-\frac{1}{12n^2\alpha}-\alpha\ln(\alpha)+\frac{1}{2}\ln(\alpha)-\frac{1}{12\alpha}\\
&=\alpha\ln(n)-\left(\frac{1}{2n}-\frac{1}{12n^2}-\frac{1}{12}\right)\frac{1}{\alpha}+\frac{1}{2}\ln(\alpha).
\end{aligned}
$$

Por último, considerando que $\mathbb{E}_p[\frac{1}{\alpha}]\approx \frac{2(n\ln(\tau/n)-\theta)}{n+1}$ e $\mathbb{E}_p[\ln(\alpha)]\approx \psi\left(\frac{n+3}{2}\right)-\ln(n\ln(\tau/n)-\theta)$ (pois $\alpha$ tem distribuição aproximada $\mathcal{G}\left(\frac{n+3}{2},n\ln(\tau/n)-\theta\right)$), então:

$$
\mathbb{E}_p[\alpha\ln(\beta)-\ln(\Gamma(\alpha))] \approx \frac{n+3}{2(n\ln(\tau/n)-\theta)}\ln(n)-\left(\frac{1}{2n}-\frac{1}{12n^2}-\frac{1}{12}\right)\frac{2(n\ln(\tau/n)-\theta)}{n+1}+\frac{1}{2}\psi\left(\frac{n+3}{2}\right)-\ln(n\ln(\tau/n)-\theta).
$$

Usando a equação acima, ainda temos de usar Newton-Raphson para resolver o sistema, porém com estas simplificações o algoritmo ficou muito mais rápido e converge sem dificuldades, possibilitando a compatibilização das prioris. Vale destacar também que a aproximação usada é muito boa, de modo que não há perdas significativas na resolução do sistema (pelo menos nos casos em que testei).

Usando os resultados acima, foi possível fazer o ajuste do GDLM, prossigamos então a análise apresentando o resultado do ajuste do modelo usando as aproximações propostas.

Para testas a qualidade do ajuste, geramos uma amostra i.i.d. com 200 elemento da distribuição $\mathcal{G}(1,1)$ e usamos a abordagem do artigo k-paramétrico para estimar os parâmetros $\alpha$ e $\beta$ da distribuição dos dados. O resultado pode ser observado a seguir:

```{r}
devtools::load_all()

# Gamma case
set.seed(15315)
T <- 200
a=exp(log(10)+cumsum(rnorm(T,0,0.1)))
b=a/(exp(log(5)+cumsum(rnorm(T,0,0.1))))
outcome <- matrix(rgamma(T,a, b), T, 1)

# s=max(outcome)#mean(10*outcome)
# outcome=outcome/s

level <- polynomial_block(order = 1, values = 1, D = 1 / 0.9,k=2,C0=1)
season=harmonic_block(period = 20, values = c(0,1), D = 1 / 1,by_time = FALSE,C0=1)

fitted_data <- fit_model(level, outcome = outcome, family = "FGamma")

sample=FFBS_sampling(fitted_data,10000)
params=rowMeans(sample$param[,T,])

mu=rep(NA,T)
ic_up=rep(NA,T)
ic_down=rep(NA,T)

for(t in 1:T){
  mu[t]=mean(sample$param[2,t,])
  ic_up[t]=mean(qgamma(0.975,sample$param[1,t,],sample$param[1,t,]/(sample$param[2,t,])))
  ic_down[t]=mean(qgamma(0.025,sample$param[1,t,],sample$param[1,t,]/(sample$param[2,t,])))
}
```

```{r}
plot(outcome, main='Distribuição preditiva',ylim=c(0,20))
lines(1:T,mu)
lines(1:T,ic_up,lty=2)
lines(1:T,ic_down,lty=2)
legend('topright',legend=c('Observações','Média','I.C. 95%'),lty=c(NA,1,2),pch=c(1,NA,NA))



cat(paste0('Parâmetro phi\n',
           'Valor real: ',a,'\n',
           'Média: ',mean(sample$param[1,T,]),'\n',
           'Mediana: ',median(sample$param[1,T,]),'\n',
           'Quantil de 0.975: ',quantile(sample$param[1,T,],0.975),'\n',
           'Quantil de 0.025: ',quantile(sample$param[1,T,],0.025),'\n'))
cat(paste0('Parâmetro mu\n',
           'Valor real: ',a/b,'\n',
           'Média: ',mean(sample$param[2,T,]),'\n',
           'Mediana: ',median(sample$param[2,T,]),'\n',
           'Quantil de 0.975: ',quantile(sample$param[2,T,],0.975),'\n',
           'Quantil de 0.025: ',quantile(sample$param[2,T,],0.025),'\n'))
```

Podemos observar que a estimativa dos parâmetros está bem ruim. Afim de tentar entender a causa para essas estimativas ruins, apresentamos um gráfico com a estimação dos parâmetros a cada iteração:

```{r}
plot(1:T,fitted_data$conj_prior_param$n[1]+1:T,lty=2,type='l',xlab='Tempo',ylab='$n_0$' %>% TeX,main='Valores do parâmetro n0 da distribuição conjugada')
lines(1:T,fitted_data$conj_post_param$n)
legend('topleft',legend=c('Método KL','Solução analítica'),lty=c(1,2))
```
```{r}
plot(1:T,fitted_data$conj_prior_param$k[1]+1:T,lty=2,type='l',xlab='Tempo',ylab='$n_0$' %>% TeX,main='Valores do parâmetro k0 da distribuição conjugada')
lines(1:T,fitted_data$conj_post_param$k)
legend('topleft',legend=c('Método KL','Solução analítica'),lty=c(1,2))
```

```{r}
plot(1:T,fitted_data$conj_prior_param$tau[1]+cumsum(outcome),lty=2,type='l',xlab='Tempo',ylab='$\tau_0$' %>% TeX,main='Valores do parâmetro tau0 da distribuição conjugada')
lines(1:T,fitted_data$conj_post_param$tau)
legend('topleft',legend=c('Método KL','Solução analítica'),lty=c(1,2))
```

```{r}
plot(1:T,fitted_data$conj_prior_param$theta[1]+cumsum(log(outcome)),lty=2,type='l',xlab='Tempo',ylab='$\theta_0$' %>% TeX,main='Valores do parâmetro theta0 da distribuição conjugada',ylim=c(0,200))
lines(1:T,fitted_data$conj_post_param$theta)
legend('bottomleft',legend=c('Método KL','Solução analítica'),lty=c(1,2))
```

Claramente temos um problema na compatibilização das prioris, especificamente, parece que a informação adquirida após se observar o dado é perdida durante a compatibilização. Mais investigações serão feitas para tentar resolver esse problema.
