---
title: "Relatório sobre a qualidade das aproximações"
author: "Silvaneo Viera dos Santos Junior"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	fig.height=8, fig.width=12
)
library(ggplot2)
library(plotly)
library(rootSolve)
library(latex2exp)
library(tidyverse)
library(gridExtra)

devtools::load_all()

line_width=1.5
seed=21351
```

## Introdução

Neste relato apresento os resultados dos experimentos que fiz para testar a qualidade das aproximações usadas no artigo sobre GDLM k-paramétricos.

Na primeira seção, será apresentado o problema sobre a qualidade da aproximação de primeira ordem da função digamma ($\psi$) e duas aproximações alternativas com precisão satisfatória. No final da análise concluimos que é necessário o uso de ordem superior a 1, desta forma, usaremos a aproximação de segunda ordem para a função *digamma* sempre que necessário.

Na seção seguinte, discutiremos a qualidade da aproximação usada para o modelo com resposta Poisson para diversos valores do parâmetro de taxa (ou média). Apresentaremos também exemplos de ajustes para valores "extremos" do parâmetro de taxa afim de encontrar um conjunto onde o uso do modelo é sempre válido. Com as análises desta seção, concluímos que o ajuste é bom no caso Poisson com média superior a 1 e razoável para Poisson's com média acima de 0.75.

Na 3ª seção abordaremos a qualidade da aproximação para o modelo com resposta Gamma com parâmetro de forma ($\phi$) conhecido. De forma análoga ao caso Poisson, avaliaremos o ajuste para diversos valores do parâmetro de forma (a média do dado não afeta a qualidade do ajuste) e encontramos um conjunto onde o ajuste é sempre válido. No final desta análise, concluímos que o ajuste do modelo é bom para os casos Gamma com $\phi$ superior a 0.75 e satisfatório para casos Gamma com $\phi$ superior a 0.5.

Na 4ª seção discutiremos a qualidade da aproximação usada para o modelo com resposta Multinomial para diversos valores dos parâmetros $N$ e $p$. Para este caso, avaliaremos especificamente o caso binomial, tendo em mente que o resultado da análise se extende aos casos com mais de 2 categorias. Ao final da análise, concluimos que o ajuste é bom para uma certa categoria da multinomial se o valor esperado de observações para esta categoria for maior ou igual a 0.2.

Por último, discutiremos a qualidade da aproximação para o modelo com resposta Normal com variância desconhecida. Este é o caso mais delicado, pois a qualidade do ajuste **não** depende da distribuição dos dados (mostraremos o porquê), desta forma, este é o caso onde o ajuste do modelo se mostra mais discrepante com a solução analítica. Posto isto, o ajuste ainda me parece satisfatório, ademais, é possível estabelecer condições para que a compatibilização das distribuições seja válida (estas condições dependem da estrutura do modelo, mas não da distribuição do dado). Alternativamente, também apresento uma proposta que melhora a qualidade do ajuste e não impõe nenhuma restrições na estrutura do modelo.

\pagebreak

## Qualidade da aproximação para a função digamma

Ao montar os sistemas que devem ser resolvidos para obter a priori conjugada, frequentemente nos deparamos com a função digamma ($\psi$). Por exemplo, no caso do modelo com reposta Poisson, adotamos uma priori Gamma; para encontrar os parâmetros da distribuição Gamma que melhor aproxima a distribuição do preditor linear (que é assumida como normal), devemos encontrar $\tau_1=\alpha$ e $\tau_2=\beta$ tais que:


\begin{equation}
\label{sis:1}
\begin{aligned}
\psi(\alpha)-\ln(\beta) =& f_t\\
\frac{\alpha}{\beta} =& \exp\{f_t+q_t/2\},
\end{aligned}
\end{equation}

onde $f_t$ é a média do preditor linear e $q_t$ é a sua variância.

Para resolver esse sistema sem recorrer a método numéricos, usamos que:

$$
\psi(x)=\ln(x)-\frac{1}{2x}-\frac{1}{12x^2}+\frac{1}{120x^4}-\frac{1}{252x^6}+\dots
$$


Assim, uma aproximação de primeira ordem para $\psi(x)$ seria $\tilde{\psi}_1(x)=\ln(x)-\frac{1}{2x}$. Com esta aproximação, o sistema (\ref{sis:1}) tem a seguinte solução:

$$
\begin{aligned}
\alpha =& q_t^{-1}\\
\beta =& \alpha\exp\{-f_t-q_t/2\}\\
\end{aligned}
$$

De forma análogoa, uma aproximação de segunda ordem para $\psi(x)$ seria $\tilde{\psi}_2(x)=\ln(x)-\frac{1}{2x}-\frac{1}{12x^2}$. Com esta aproximação, o sistema (\ref{sis:1}) tem a seguinte solução:

$$
\begin{aligned}
\alpha =& \frac{1}{-3+3\sqrt{1+\frac{2}{3}q_t}}\\
\beta =& \alpha\exp\{-f_t-q_t/2\}\\
\end{aligned}
$$

No gráfico a seguir, apresentamos uma comparação entre as aproximações de primeira e segunda ordem e tomando como valor "exato" de $\psi$ os valores fornecidos pela função \emph{digamma} do \emph{R}:

```{r }
x=seq(0,2,l=1000)
y_true=digamma(x)
y_approx1=log(x)-1/(2*x)
y_approx2=log(x)-1/(2*x)-1/(12*(x**2))

p1=ggplot()+
    geom_line(aes(x=x,y=y_true,color='Valor "exato"'))+
    geom_line(aes(x=x,y=y_approx1,color='Aproximação de 1ª ordem'),linetype='dashed')+
    geom_line(aes(x=x,y=y_approx2,color='Aproximação de 2ª ordem'),linetype='dashed')+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    coord_cartesian(ylim=c(-10,1))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$\\psi(x)$' %>% TeX,expand=c(0,0))+
    scale_color_manual('',values=c('#ff8822','#5555ff','black'))+
    theme_bw()
  
p1
```

Veja que, para valores de $x$ inferiores a $1$, a aproximação de primeira ordem deixa muito a desejar. Já a aproximação de segunda ordem, apesar de também se afastar do valor "exato", tem uma precisão muito maior.

A seguir apresentamos uma comparação entre as soluções obtidas por cada aproximação para o sistema (\ref{sis:1}) usando como valor de referência a solução obtida com o método de Newton-Raphson para o sistema original com tolerância de $1e-20$ (isto é, a solução obtida está na vizinhança de $1e-20$ da solução exata):


```{r}
qt=seq(0,5,l=10000)
alpha_true=rep(NA,length(qt))
for(i in 1:length(qt)){
  root=multiroot(function(alpha){digamma(exp(alpha))-log(exp(alpha))+qt[i]/2},start=0,rtol=1e-20,maxiter=1000)
  alpha_true[i]=exp(root$root)
}

alpha_approx1=1/qt
alpha_approx2=1/(-3+3*sqrt(1+2*qt/3))
```

```{r}
p1=ggplot()+
    geom_line(aes(x=qt,y=alpha_true,color='Solução por\nNewton-Raphson'))+
    geom_line(aes(x=qt,y=alpha_approx1,color='Aproximação\nde 1ª ordem'),linetype='dashed')+
    geom_line(aes(x=qt,y=alpha_approx2,color='Aproximação\nde 2ª ordem'),linetype='dashed')+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    coord_cartesian(ylim=c(0,10))+
    scale_x_continuous('$q_t$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$\\alpha$' %>% TeX)+
    scale_color_manual('',values=c('#ff8822','#5555ff','black'))+
    theme_bw()

mini_range=list('x'=c(3.5,4.5),
                'y'=c(0,2))
mini_place=list('x'=c(1,4),
                'y'=c(2.5,8.5))

p2=ggplot()+
    geom_line(aes(x=qt,y=alpha_true,color='Solução por\nNewton-Raphson'))+
    geom_line(aes(x=qt,y=alpha_approx1,color='Aproximação\nde 1ª ordem'),linetype='dashed')+
    geom_line(aes(x=qt,y=alpha_approx2,color='Aproximação\nde 2ª ordem'),linetype='dashed')+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    scale_color_manual('',values=c('#ff8822','#5555ff','black'))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))

p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = mini_place$x[1],
                xmax = mini_place$x[2],
                ymin = mini_place$y[1],
                ymax = mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2]) +
  geom_rect(aes(xmin = mini_range$x[1],
                xmax = mini_range$x[2],
                ymin = mini_range$y[1],
                ymax = mini_range$y[2]), color='black', linetype='solid', alpha=0) 
```

Observe que, para valores suficientemente pequenos de $q_t$, as duas soluções são praticamente iguais, mas para valores maiores de $q_t$ a aproximação de 1ª ordem deixa muito a desejar, enquanto a aproximação de 2ª ordem é bem próxima da solução de referência.

Adiante apresentaremos o resultado do ajuste de um modelo dinâmico Poisson para cada uma das aproximações. Os ajustes foram feitos com amostras i.i.d. de Poisson's com taxas 1, 10, 20 e 200. Para cada amostra, ajustamos um modelo dinâmico Poisson com estrutura de primeira ordem para o nível e com fator de desconto igual a $1$. Neste cenário, temos solução analítica para a estimação dos parâmetros, assim, podemos comparar os parâmetros estimados pelo modelo com a solução analítica. Como referência para a qualidade da aproximação da função $\psi$, também ajustamos 2 modelos usando o método de Newton-Raphson para resolver o sistema (\ref{sis:1}). Inicialmente, calculamos as soluções com Newton-Raphson usando tolerância de $1e-6$ para a solução, porém este valor se mostrou inadequado, o que nos levou a fazer um ajuste alternativo com tolerância de $1e-20$ para a solução. Por último, no gráfico a seguir, vamos nos referir a abordagem proposta no artigo sobre GDLM k-paramétricos como KL (ou aproximação KL, em alguns casos), sendo que vamos nos referir aos casos onde essa abordagem foi usada junto com o método de Newton-Raphson para a resolução do sistema (\ref{sis:1}) como KL com N-R.

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

T=50
for(lambda in c(1,10,20,200)){

  set.seed(seed)
  y=rpois(T,lambda)
  
  
  level=polynomial_block(order=1,values=1,D=1/1)
  
  resultado1=fit_model(level,
                      outcome = y,
                      family='poisson_ord1')
  
  resultado2=fit_model(level,
                      outcome = y,
                      family='poisson')
  
  resultado_rn_defaul=fit_model(level,
                      outcome = y,
                      family='poisson_rn_default')
  
  resultado_true=fit_model(level,
                      outcome = y,
                      family='poisson_true')
  
  a=resultado_true$conj_prior_param[1,1]
  b=resultado_true$conj_prior_param[1,2]
  
  a.post=a+cumsum(y)
  b.post=b+1:T
  
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=a.post,
                             lambda=paste0('$\\lambda=',lambda,'$'),
                             color='Sol. Analítica'))
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=resultado_rn_defaul$conj_post_param[,1],
                             lambda=paste0('$\\lambda=',lambda,'$'),
                             color='KL com N-R tol=1e-6'))
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=resultado_true$conj_post_param[,1],
                             lambda=paste0('$\\lambda=',lambda,'$'),
                             color='KL com N-R tol=1e-20'))
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=resultado1$conj_post_param[,1],
                             lambda=paste0('$\\lambda=',lambda,'$'),
                             color='KL com aprox. 1ª ordem'))
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=resultado2$conj_post_param[,1],
                             lambda=paste0('$\\lambda=',lambda,'$'),
                             color='KL com aprox. 2ª ordem'))
}
```

```{r}
(ggplot(data_plot)+
    geom_line(aes(x=time,y=alpha,color=color,linetype=as.character(color %in% c('Sol. Analítica','KL com N-R tol=1e-6','KL com N-R tol=1e-20'))))+
  guides(linetype=FALSE)+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
    scale_color_manual('',values=c('#ff8822','#5555ff','#22aa22','#8822ff','black'))+
    ylab('$\\alpha$' %>% TeX)+
    theme_bw()+
  facet_wrap(~lambda,scales ='free'))
```

Observe que a solução KL N-R com tolerância $1e-6$ deixa bastante a desejar quando $\alpha$ é grande, sendo necessário maior precisão numérica para a solução. A solução KL N-R com tolerância $1e-20$, apesar de boa, tem um custo computacional elevado, sendo necessárias várias iterações do algoritmo para obter uma solução com a precisão desejada. Em contra-partida, a solução KL com aproximação de 2ª produz um resultado essencialmente igual ao KL N-R com tolerância $1e-20$, mas sem a necessidade de métodos numéricos para a solução. Vale observar que, apesar das soluções encontradas para $\lambda$ pequeno serem consideravelmente diferentes da solução analítica, veremos na próxima seção que esta discrepância no espaço paramétrico **não** se traduz em uma grande discrepância de ajuste do modelo.

Por último, antes de finalizar esta seção, acredito que seja válido discutir uma possível alternativa para a aproximação de 2ª ordem de $\psi$. Sabemos que a função $\psi$ tem a seguinte propriedade:

$$
\psi(x)=\psi(x+1)-\frac{1}{x}
$$

Como aproximação de 1ª ordem é boa para valores de $x$ maiores do que $1$, podemos usar uma aproximação de primeira ordem para $\psi(x+1)$ e encontrar uma aproximação muito boa para $\psi$, de fato, a aproximação obtida é:

$$
\psi(x)\approx \ln(x+1)-\frac{1}{2(x+1)}-\frac{1}{x}
$$

No gráfico a seguir comparamos a aproximação de 1ª ordem original com a aproximação melhorada:

```{r}
x=seq(0,2,l=1000)
y_true=digamma(x)
y_approx1=log(x)-1/(2*x)
y_approx2=log(x+1)-1/(2*(x+1))-1/x

p1=ggplot()+
    geom_line(aes(x=x,y=y_true,color='Valor "exato"'))+
    geom_line(aes(x=x,y=y_approx1,color='Aproximação de 1ª ordem'),linetype='dashed')+
    geom_line(aes(x=x,y=y_approx2,color='Aproximação de 1ª ordem\nmelhorada'),linetype='dashed')+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    coord_cartesian(ylim=c(-10,1))+
    scale_x_continuous('x',expand=c(0,0))+
    scale_y_continuous('digamma(x)',expand=c(0,0))+
    scale_color_manual('',values=c('#ff8822','#22aa22','black'))+
    theme_bw()

mini_range=list('x'=c(0.1,0.4),
                'y'=c(-5,-3))
mini_place=list('x'=c(0.75,0.75+5*0.3/2),
                'y'=c(-7,-2))

p2=ggplot()+
    geom_line(aes(x=x,y=y_true,color='Valor "exato"'))+
    geom_line(aes(x=x,y=y_approx1,color='Aproximação de 1ª ordem'),linetype='dashed')+
    geom_line(aes(x=x,y=y_approx2,color='Aproximação de 1ª ordem\nmelhorada'),linetype='dashed')+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    scale_color_manual('',values=c('#ff8822','#22aa22','black'))+
    guides(color=FALSE)+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))

p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = mini_place$x[1],
                xmax = mini_place$x[2],
                ymin = mini_place$y[1],
                ymax = mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2]) +
  geom_rect(aes(xmin = mini_range$x[1],
                xmax = mini_range$x[2],
                ymin = mini_range$y[1],
                ymax = mini_range$y[2]), color='black', linetype='solid', alpha=0) 
```

Veja que esta aproximação é muito melhor do que a aproximação de 2ª ordem! Porém, esta aproximação ajuda em muito pouco na resolução do sistema (\ref{sis:1}), ademais, como já vimos, não há ganho significativo em melhorar a aproximação de segunda ordem uma vez que a solução obtida com esta aproximação é comparável à solução obtida por N-R com tolerância $1e-20$.


\pagebreak

## Qualidade da compatibilização de priori: Caso Poisson

Primeiramente, vamos apresentar modelo que estamos usando para fixar a notação. Sejam $y$ uma v.a. tal que:

$$
\begin{aligned}
y|\eta& \sim Poisson(\eta)\\
\ln(\eta)& = \lambda\\
\lambda&\sim \mathcal{N}(f,q).
\end{aligned}
$$

Nesta notação, $\eta$ é o parâmetro de taxa da Poisson e $\lambda$ é o preditor linear.

Na abordagem KL, para obter a priori conjugada para $\eta$ ($\eta \sim \mathcal{G}(\tau_1,\tau_2)$), devemos resolver o seguinte sistema:

\begin{equation}
\label{sis:2}
\begin{aligned}
\psi(\tau_1)-\ln(\tau_2) =& f\\
\frac{\tau_1}{\tau_2} =& \exp\{f+q/2\},
\end{aligned}
\end{equation}

Usando a aproximação de segunda ordem para a função $\psi$, obtemos a seguinte solução para o sistema:

\begin{equation}
\label{sol:2}
\begin{aligned}
\tau_1 =& \frac{1}{-3+3\sqrt{1+\frac{2}{3}q}}\\
\tau_2 =& \tau_1\exp\{-f-q/2\},
\end{aligned}
\end{equation}

Obtidos $\tau_1$ e $\tau_2$, definimos $\tau^{*}_1=\tau_1+y$ e $\tau^{*}_2=\tau_2+1$ de modo que $\eta|y \sim \mathcal{G}(\tau^{*}_1,\tau^{*}_2)$

Para obter a posteriori Normal para $\lambda$ ($\lambda|y \sim \mathcal{N}(f^{*},q^{*})$), devemos tomar $f^{*}$ e $q^{*}$ tais que:

\begin{equation}
\label{sis:3}
\begin{aligned}
f^{*} =& \psi(\tau^{*}_1)-\ln(\tau^{*}_2)\\
q^{*} =& \psi'(\tau^{*}_1).
\end{aligned}
\end{equation}

Uma dúvida a ser levantada é se a aproximação das distribuições usando a metodologia proposta não induz algum tipo de "distorção" na estimação. Uma forma de se averiguar isso seria observar o que ocorre com com os parâmetros da Normal após a compatibilização quando não atualizamos a distribuição de $\eta$, isto é, se começamos com uma distribuição $\mathcal{N}(f,q)$ para $\lambda$, obtemos a distribuição conjugada $\mathcal{G}(\tau_1,\tau_2)$ para $\eta$ e depois obtemos uma nova distribuição $\mathcal{N}(f^{*},q^{*})$ para $\lambda$ sem atualizar a distribuição de $\eta$, o quanto $\mathcal{N}(f,q)$ difere de $\mathcal{N}(f^{*},q^{*})$?

Para responder esta pergunta podemos observar que:
$$
f^{*}=\psi(\tau^{*}_1)-\ln(\tau^{*}_2)=\psi(\tau_1)-\ln(\tau_2)=f.
$$

Logo, a menos de desvios devidos a aproximação de $\psi$, as duas normais tem a mesma média. Para o valor de $q^*$, observe que:

$$
q^{*}=\psi'(\tau^{*}_1)=\psi'(\tau_1)=\psi'\left(\frac{1}{-3+3\sqrt{1+\frac{2}{3}q}}\right).
$$

Avaliaremos a seguir o gráfico de $q$  contra $q^{*}$ para verificar se há alguma distorção do valor original de $q$ após passar pelas aproximações:

```{r}
inter_x=function(x){trigamma(1/(-3+3*sqrt(1+2*x/3)))}
x=seq(0,10,l=1000)

p1=ggplot()+
    geom_line(aes(x=x,y=inter_x(x),color='Aproximação'))+
    geom_line(aes(x=x,y=x,color='Identidade'),linetype='dashed')+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_x_continuous('$q$' %>% TeX)+
    scale_y_continuous('$q^*$' %>% TeX)+
    scale_color_manual('',values=c('#5555ff','black'))+
    coord_cartesian(ylim=c(0,10))+
    theme_bw()

mini_range=list('x'=c(0,1),
                'y'=c(0,1))
mini_place=list('x'=c(5,9),
                'y'=c(1,5))

p2=ggplot()+
    geom_line(aes(x=x,y=inter_x(x),color='Aproximação'))+
    geom_line(aes(x=x,y=x,color='Identidade'),linetype='dashed')+
  guides(color=FALSE)+
    scale_color_manual('',values=c('#5555ff','black'))+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_x_continuous('$q$',expand=c(0,0))+
    scale_y_continuous('$q^*$' %>% TeX,expand=c(0,0),lim=c(0,10))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))

p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = mini_place$x[1],
                xmax = mini_place$x[2],
                ymin = mini_place$y[1],
                ymax = mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2]) +
  geom_rect(aes(xmin = mini_range$x[1],
                xmax = mini_range$x[2],
                ymin = mini_range$y[1],
                ymax = mini_range$y[2]), color='black', linetype='solid', alpha=0) 
```

Observe que $q^{*}>q$, ademais a discrepância entre $q$ e $q^{*}$ **parece** crescer exponencialmente. Para tentar entender o comportamente desta aproximação, especialmente nos casos onde $q\approx 0$ e $q \approx +\infty$, podemos olhar para a razão $\frac{q^{*}}{q}$ contra $q$ na escala $\log$ x $\log$:

```{r}
inter_x=function(x){trigamma(1/(-3+3*sqrt(1+2*x/3)))}
x=exp(seq(-15,15,l=1000))

p1=ggplot()+
    geom_line(aes(x=log(x),y=log(inter_x(x)/x)))+
    geom_line(aes(x=c(log(0.25),log(1)),y=c(log(inter_x(1)/1),log(inter_x(1)/1))))+
    geom_label(aes(x=log(0.25),y=log(inter_x(1)/1),label = round((inter_x(1))*100,2) %>% paste0('%')))+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    labs(title='Valor relativo da variância após a aproximação')+
    scale_x_continuous('$q$' %>% TeX,expand=c(0,0),lim=c(log(1e-6),log(1e+6)),breaks=log(10**c(-6:6)),labels=function(x){x %>% exp %>% round(6)})+
    scale_y_continuous('$q^{*}/q$' %>% TeX,lim=c(0,log(6)),breaks=log(c(1:6)),labels=function(x){paste0(exp(x)*100,'%')})+
    theme_bw()

p1
```

Veja que o aumento percentual da variância converge para $500\%$ quando $q \rightarrow +\infty$. Se por um lado $500\%$ é um valor bem alto, pelo menos esta razão não diverge para $+\infty$. Vale destacar também que este aumento na variância é razoavelmente alto mesmo para valores usuais de $q$, por exemplo, para $q=1$, temos um aumento de $35.3\%$ na variância da distribuição aproximada.

Vale notar que $q$ é inversamente proporcional à $\tau_1$, desta forma, a aproximação ser boa para valores pequenos $q$ é o mesmo que dizer que a aproximação é boa para valores grande de $\tau_1$. Desta forma a qualidade da apoximação depende de que $\tau_1$ seja suficientemente grande.

Na prática, a dependência no valor de $\tau_1$ é pouco impeditiva, pois a cada iteração do algoritmo devemos atualizar $\tau_1$ para $\tau^{*}_1=\tau_1+y$, assim, mesmo que comecemos com um valor de $\tau_1$ baixo, após algumas iterações teremos uma aproximação suficientemente boa para que a abordagem funcione bem. Dito isso, ainda há perda de informação na transição entre as distribuições. Suponha que estamos no caso onde, a cada iteração, perdemos exatamente $80\%$ o valor de $\tau_1$ no processo de compatibilização, ao obter $\tau^*_{1}=\tau_1+y$ aproveitaremos na próxima iteração apenas $20\%$ do valor observado de $y$, isso vai fazer com que o valor de $\tau_1$ cresça numa velocidade inferior ao que seria "exato". Contudo, se $y$ for suficientemente grande, mesmo aproveitando apenas $20\%$ do valor observado, a atualização pode "deslocar" o parâmetro $\tau_1$ para uma região onde a perda de informação seja pequena. Assim, se a média de $y$ é suficientemente grande, o ajuste ficará bom para qualquer $\tau_1$ inicial. Resta então a seguinte questão: Qual o menor valor médio de $y$ que garanta um bom ajuste do modelo? O restante desta seção será dedicado a tentar responder esta pergunta.

Primeiro teste: Utilizando amostras i.i.d. de distribuições Poisson com diversas médias, vamos comparar a estimativa dos parâmetros fornecida pela abordagem KL com o valor analítico dos parâmetros. Para cada tempo $t$, temos que a distribuição a posteriori analítica para $\eta$ é:

$$
\begin{aligned}
\eta|\mathcal{D}_t \sim& \mathcal{G}(\alpha_{t},\beta_{t})\\
\alpha_t=&\alpha_{t-1}+y_t\\
\beta_t=&\beta_{t-1}+1,
\end{aligned}
$$
sendo que $\alpha_0$ e $\beta_0$ foram escolhidos de modo que a solução analítica e a abordagem KL tenham a mesma priori.

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

T=50
for(lambda in c(0.5,1,2,10,20,200)){

  set.seed(seed)
  y=rpois(T,lambda)
  
  
  level=polynomial_block(order=1,values=1,D=1/1)
  
  resultado=fit_model(level,
                      outcome = y,
                      family='poisson')
  
  a=resultado$conj_prior_param[1,1]
  b=resultado$conj_prior_param[1,2]
  
  a.post=a+cumsum(y)
  b.post=b+1:T
  
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=a.post,
                             beta=b.post,
                             lambda=paste0('$\\eta=',lambda,'$'),
                             color='Sol. Analítica'))
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             alpha=resultado$conj_post_param[,1],
                             beta=resultado$conj_post_param[,2],
                             lambda=paste0('$\\eta=',lambda,'$'),
                             color='Aproximação KL'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=time,y=alpha,color=color,linetype=as.character(color == 'Sol. Analítica')))+
  guides(linetype=FALSE)+
   geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
    ylab('$\\alpha$' %>% TeX)+
    labs(title='Parametro $\\alpha$ da distribuicao de $\\eta|D_t$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(lambda,levels=unique(data_plot$lambda)),scales ='free'))
```

Observe no gráfico acima que, quando a linha tracejada (sol. analítica) se mantém estável (isto é $y_i=0$), o valor de $\alpha$ diminui. Isso faz com que a presença de 0's na amostra prejudique a qualidade do ajuste. Dito isso, se a frequência com que 0's ocorrem é baixa, então não há problemas.

```{r}
(ggplot(data_plot)+
    geom_line(aes(x=time,y=beta,color=color,linetype=as.character(color == 'Sol. Analítica')))+
  guides(linetype=FALSE)+
   geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
    ylab('$\\beta$' %>% TeX)+
    labs(title='Parametro $\\beta$ da distribuicao de $\\eta|D_t$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(lambda,levels=unique(data_plot$lambda)),scales ='free'))
```

No gráfico acima podemos ver um resultado análogo a estimação de $\alpha$.

Observe que, a partir de $\eta=10$ temos que a aproximação KL gera resultado bem próximos da solução analítica. Dito isso, vale destacar que estamos olhando a distância entre a aproximação KL e a solução analítica no espaço paramétrico, mas uma grande distância no espaço paramétrico não necessariamente implica que as distribuições são de fato muito diferentes.

A seguir vamos exibir comparações entre a distribuição de $\eta$ antes e depois da compatibilização para diversos valores de $\alpha=\tau_1$. Em cada um dos casos apresentado a frente, começamos com $\eta \sim \mathcal{G}(\tau_1,\tau_2)$, depois obtemos a distribuição aproximada Normal para o $\lambda$ e por fim usamos a distribuição de $\lambda$ para encontrar uma nova distribuição $\mathcal{G}(\tau_1^*,\tau_2^*)$ para $\eta$. Avaliamos então a diferença entre a distribuição original e a distribuição final de $\eta$:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

mu=1
for(alpha in c(0.1,0.5,1,2,5,10)){
  new_norm=do.call(convert_Gamma_Normal,convert_Normal_Gamma(list('alpha'=alpha,'beta'=alpha/mu)))
  alpha_alt=new_norm$alpha
  beta_alt=new_norm$beta
  
  
  x=seq(qgamma(0.005,alpha,alpha/mu),qgamma(0.995,alpha,alpha/mu),l=1000)
  
  fx_true=dgamma(x,alpha,alpha/mu)
  fx_approx=dgamma(x,alpha_alt,beta_alt)
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=ifelse(fx_true>10,NA,fx_true),
                             alpha=paste0('$\\tau_1=',alpha,'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=ifelse(fx_approx>10,NA,fx_approx),
                             alpha=paste0('$\\tau_1=',alpha,'$'),
                             color='Distr. final'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(alpha,levels=unique(data_plot$alpha)),scales ='free',nrow=3))
```

Veja que a aproximação já parece bem razoável para $\tau_1=2$ e me parece "decente" para $\tau_1=1$. Já para valores de $\tau_1<1$, a aproximação não parece tão confiável.

Adiante exibimos uma comparação parecida, porém vamos comparar a distribuição de $\lambda$. Neste caso, começamos com a distribuição $\lambda \sim \mathcal{N}(f,q)$, obtemos a distribuição aproximada para $\eta \sim \mathcal{G}(\tau_1,\tau_2)$ e, usando a distribuição de $\eta$, obtemos uma nova distribuição aproximada para $\lambda \sim \mathcal{N}(f^*,q^*)$. Feito este processo, comparamos a distribuição original de $\lambda$ com a distribuição final:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

for(alpha in c(0.1,0.5,1,2,5,10)){
  sigma2=convert_Normal_Gamma(list('alpha'=alpha,beta=1))$Qt
  mu=convert_Normal_Gamma(list('alpha'=alpha,beta=1))$ft
  new_norm=convert_Gamma_Normal(mu,sigma2) %>% convert_Normal_Gamma
  mu_alt=new_norm$ft
  sigma2_alt=new_norm$Qt
  
  
  x=seq(mu-5*sqrt(sigma2),mu+5*sqrt(sigma2),l=1000)
  
  fx_true=dnorm(x,mu,sqrt(sigma2))
  fx_approx=dnorm(x,mu_alt,sqrt(sigma2_alt))
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=ifelse(fx_true>10,NA,fx_true),
                             alpha=paste0('$\\tau_1=',alpha,'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=ifelse(fx_approx>10,NA,fx_approx),
                             alpha=paste0('$\\tau_1=',alpha,'$'),
                             color='Distr. aprox.'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(alpha,levels=unique(data_plot$alpha)),scales ='free',nrow=3))
```

Observe que começamos a ter problemas com a média da distribuição quando $\tau_1$ é muito pequeno (como por exemplo $\tau_1=0.1$), mas, novamente, a aproximação parece razoável para $\tau_1\ge1$.

A partir do gráficos acima, temos indícios de que o ajuste do modelo é válido para Poisson's com taxa pelo menos $1$, sendo irrazoável para Poisson's com taxas muito baixas.

Dito isso, a análise se baseia em apenas uma iteração da compatibilização, sendo assim, para avaliar o comportamente de diversas iterações consecutivas da compatibilização, faremos o ajuste com dados simulados.

Nos gráficos a seguir, mostraremos o ajuste feito de forma analítica e com a aproximação KL para amostras i.i.d. de Poisson's com diversas médias. Assim como no primeiro teste que fizemos, a solução análitica para o processo de filtragem de $\eta$ é dada por:

$$
\begin{aligned}
\eta|\mathcal{D}_t \sim& \mathcal{G}(\alpha_{t},\beta_{t})\\
\alpha_t=&\alpha_{t-1}+y_t\\
\beta_t=&\beta_{t-1}+1,
\end{aligned}
$$

sendo que $\alpha_0$ e $\beta_0$ foram escolhidos de modo que a solução analítica e a abordagem KL tenham a mesma priori.

```{r fig.height=8}
T=200
p_list=list()
for(lambda in c(0.5,0.6,0.75,1,2,5)){
set.seed(seed)
y=rpois(T,lambda)


level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = y,
                    family='poisson')

a=resultado$conj_prior_param[1,1]
b=resultado$conj_prior_param[1,2]

a.post=a+cumsum(y)
b.post=b+1:T

data1=data.frame(time=1:T,
           mean=a.post/b.post,
           median=(a.post-1)/b.post,
           C.I.lower=qgamma(0.025,a.post,b.post),
           C.I.upper=qgamma(0.975,a.post,b.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1]/resultado$conj_post_param[,2],
           median=(resultado$conj_post_param[,1]-1)/resultado$conj_post_param[,2],
           C.I.lower=qgamma(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=qgamma(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  coord_cartesian(ylim=c(0,4*lambda))+
    guides(color=FALSE,linetype=FALSE)+
  scale_linetype_manual('',values=c('dashed','solid'))+
  scale_color_manual('',values=c('#5555ff','black'))+
   geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Filtragem' %>% paste(paste0('($\\eta=',lambda,'$)')) %>% TeX)+
  theme_bw()

size_dist=2*lambda

mini_range=list('x'=c(150,200),
                'y'=c(0.5*lambda,1.5*lambda))
mini_place=list('x'=c(75,75+100),
                'y'=c(2*lambda,4*lambda))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
   geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(lambda>=0.75){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[lambda %>% as.character]]=p1
}
p_list$ncol=3
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Para a filtragem, temos um resultado satisfatório a partir de $\phi \ge 0.75$ e um resultado muito bom a partir de $\phi \ge 1$.

```{r}
data_plot=data.frame()
for(lambda in c(0.5,0.6,0.75,1,2,5)){
set.seed(seed)
y=rpois(T,lambda)


level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = y,
                    family='poisson')

a=resultado$conj_prior_param[1,1]
b=resultado$conj_prior_param[1,2]

a.post=a+sum(y)
b.post=b+T

data1=data.frame(mean=(a.post/b.post)-lambda,
           C.I.lower=qgamma(0.025,a.post,b.post)-lambda,
           C.I.upper=qgamma(0.975,a.post,b.post)-lambda,
           color='Sol. analítica',
           lambda=lambda)

data2=data.frame(mean=(resultado$conj_post_param[T,1]/resultado$conj_post_param[T,2])-lambda,
           C.I.lower=qgamma(0.025,resultado$conj_post_param[T,1],resultado$conj_post_param[T,2])-lambda,
           C.I.upper=qgamma(0.975,resultado$conj_post_param[T,1],resultado$conj_post_param[T,2])-lambda,
           color='Aproximação KL',
           lambda=lambda)

data_plot=rbind(data_plot,data1,data2)
}
```

```{r}
ggplot(data_plot)+
  geom_errorbar(aes(x=as.factor(lambda),ymin=C.I.lower,ymax=C.I.upper,color=color,linetype=color))+
  geom_point(aes(x=as.factor(lambda),y=mean,color=color),shape=4,size=20)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('#5555ff','black'))+
  guides(linetype=FALSE)+
  scale_x_discrete('$\\eta$' %>% TeX)+
  labs(title='Desvio do valor verdadeiro de $\\eta$ no tempo final da filtragem' %>% TeX)+
  scale_y_continuous('Desvio de $\\eta$' %>% TeX)+
  theme_bw()
```

A partir dos gráficos acima, podemos observar que a aproximação KL é satisfatoriamente próxima da solução analítica a partir de $\eta=1$, sendo que, em geral, a aproximação KL é mais "conservadora", no sentido de que a amplitude dos intervalos de credibilidade é maior do que a solução analítica (não necessariamente o i.c. da solução analítica está contido no i.c. da aproximação KL). Vale destacar que, como $\tau_1$ está sendo subestimado, temos que a aproximação $KL$ é mais sensível ao dado mais recente, dando um peso maior do que o analítico para o dado recém observado, dito isso, para $\eta$ razoavelmente grande, essa diferença na estimação da média é mínima.

Ainda no contexto de qualidade de ajuste, os casos em que analisamo até o momento não são realistas, no sentido de que, se há forma analítica de se estimar os parâmetros do modelo, não há necessidade de usar aproximações. Tendo isso em mente, adiante exibiremos o ajuste em um caso onde não há solução analítica (o valor de $\eta$ tem dinâmica temporal e assume valores inferior a 1 em alguns momentos).

Amostraremos $y_1,...,y_{200}$ com as distribuições a seguir:

$$
y_i \sim Poisson\left(\frac{1}{10}\exp\left\{\sin\left(\frac{i}{25}\pi\right)-\frac{i}{20}\left(\frac{i}{200}-1\right)+1\right\}\right)
$$

Adiante exibimos o gráfico de $\eta$ ao longo do tempo junto com os valores amostrados:

```{r}
T=200

set.seed(13031998)
w <- (200 / 50) * 2 * pi
lambda=exp((sin(w * 1:T / T)))*exp(-10*(1:T/T)*((1:T/T)-1)+1)/10
y=rpois(T,lambda)

lambda=exp((sin(w * 1:T / T)))*exp(-10*(1:T/T)*((1:T/T)-1)+1)/10
ggplot()+
  scale_color_manual('',values=c('black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Dados amostrados')+
  guides(color=FALSE)+
  geom_point(aes(x=1:T,y=y,color='eta'))+
  geom_line(aes(x=1:T,y=lambda,color='eta'))+
  theme_bw()
```

Adiante, exibimos o ajuste do modelo (com estrutura compatível com os dados, i.e., um bloco harmonico para a sazonalidade e estrutura de 3ª ordem para o nível):

```{r}
level=polynomial_block(order=3,values=1,D=1/1)
season=harmonic_block(period=50,values=1,D=1/1)

resultado=fit_model(level,season,
                    outcome = y,
                    family='poisson')

show_fit(resultado,smooth=TRUE,dynamic_plot = FALSE)$plot+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('black'))+
  scale_fill_manual('',values=c('black'))
```

No gráfico a seguir, comparamos o valor verdadeiro com o valor estimado de $\eta$ (durante a filtragem e após a suavização):

```{r}
p1=ggplot()+
  geom_line(aes(x=1:T,y=resultado$conj_post_param[,1]/resultado$conj_post_param[,2],color='Valor estimado'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qgamma(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
                  ymax=qgamma(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
                  color='Valor estimado'),alpha=0,linetype='dashed')+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Filtragem')+
  guides(color=FALSE)+
  geom_line(aes(x=1:T,y=lambda,color='Valor verdadeiro'))+
  theme_bw()
```


```{r}
alpha=rep(NA,T)
beta=rep(NA,T)
for(i in 1:T){
  ft=t(resultado$FF[,,i]) %*% resultado$mts[,i]
  Qt=t(resultado$FF[,,i]) %*% resultado$Cts[,,i] %*% resultado$FF[,,i]

  alpha[i]=1/(-3+3*sqrt(1+2*Qt[1,1]/3))
  beta[i]=alpha[i]*exp(-ft[1]-Qt[1,1]/2)
}

p2=ggplot()+
  geom_line(aes(x=1:T,y=alpha/beta,color='Valor estimado'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qgamma(0.025,alpha,beta),
                  ymax=qgamma(0.975,alpha,beta),
                  color='Valor estimado'),alpha=0,linetype='dashed')+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\eta$' %>% TeX)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Suavização')+
  guides(color=FALSE)+
  geom_line(aes(x=1:T,y=lambda,color='Valor verdadeiro'))+
  theme_bw()
```

```{r fig.height=4}
grid.arrange(p1,p2,ncol=2)
```

As curvas em preto representam o valor real do parâmetro $\eta$ a cada tempo e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Podemos observar que o modelo captura bem o comportamento dos dados e estima de forma satisfatória os valores de $\eta$ ao longo do tempo.

De modo geral, acredito que podemos concluir que a aproximação KL funciona muito bem para o ajuste de dados Poisson cuja média seja igual ou superior a $1$ na maior parte do tempo (o que acredito que engloba a maior parte das aplicações). Olhando na perpectiva do pacote para o ajuste do modelo, posso implementar *warnings* ou erros que avisam o usuário quando a $\eta$ está muito baixo e a estimação dos parâmetros foi comprometida.

Por último, para finalizar esta seção, fiz alguns testes para verificar o qual baixo $\eta$ pode ser antes de que o ajuste apresente erros numéricos. Estes erros que me refiro são **paralelos** a qualidade da estimação e ocorrem quando há uma sequência muito longa de $0$'s nas observações, o que leva o preditor linear a divergir para $-\infty$. Vale destacar que a quantidade de $0$'s consecutivos em uma amostra é aleatória, desta forma, fixado uma Poisson com taxa qualquer, há uma probabilidade positiva de que uma amostra gerada por essa Poisson cause erro numérico no ajuste. Naturalmente, para $\eta$ grande ($>0.5$, por exemplo), essa probabilidade é praticamente nula, mas foi verificado que para $\eta$ pequeno essa probabilidade é inaceitavelmente alta (para $\lambda=0.1$, por exemplo, a probabilidade de falha é de quase $80\%$).

Para verificar qual o limite para valores aceitáveis de $\eta$, rodei diversos modelos com várias taxas (de 0.1 a 0.5) e registrei o percentual de modelos onde ocorreu erro numérico. O gráfico a seguir mostra o resultado do ajuste de 2.000.000 modelos para 100 valores de $\eta$ (20.000 ajustes para cada $\eta$, cada ajuste com uma amostra de 200 elementos).

```{r eval=FALSE, include=FALSE}
library(GDLM)
T=200
level=polynomial_block(order=1,values=1,D=1/1)
L=100
N=20000
counts=rep(NA,L)
lambdas=seq(0.1,0.5,l=L)
for(j in 1:L){
  count=0
  for(i in 1:N){
    count_i=tryCatch({
      y=rpois(T,lambdas[j])
      
      
      
      resultado=fit_model(level,
                          outcome = y,
                          family='poisson')
      0
    },
    error=function(e){
      1
    },
    finally=cat(paste0(i,'        \r')))
    count=count+count_i
  }
  counts[j]=count
}

# counts
# [1] 15571 14838 14344 13683 13016 12378 11698 10846 10416  9800  9124
# [12]  8484  7801  7369  6859  6106  5672  5297  4951  4498  4101  3783
# [23]  3348  3065  2751  2495  2314  2034  1857  1651  1502  1345  1168
# [34]  1051   931   825   733   635   604   520   482   390   343   281
# [45]   272   227   224   164   144   132   131    78    94    75    61
# [56]    53    42    36    35    47    42    17    18    19    17    15
# [67]     8    10     4    12     6     7     5     5     4     2     4
# [78]     5     2     3     1     1     0     0     0     0     1     0
# [89]     0     0     0     0     0     0     0     0     0     0     0
# [100]     0
```


```{r}
# perc=c(0.777, 0.436, 0.178, 0.064, 0.016, 0.003, 0.002, 0.000, 0.000, 0.000)

perc=c(0.77855,0.74190,0.71720,0.68415,0.65080,0.61890,0.58490,0.54230,
0.52080,0.49000,0.45620,0.42420,0.39005,0.36845,0.34295,0.30530,
0.28360,0.26485,0.24755,0.22490,0.20505,0.18915,0.16740,0.15325,
0.13755,0.12475,0.11570,0.10170,0.09285,0.08255,0.07510,0.06725,
0.05840,0.05255,0.04655,0.04125,0.03665,0.03175,0.03020,0.02600,
0.02410,0.01950,0.01715,0.01405,0.01360,0.01135,0.01120,0.00820,
0.00720,0.00660,0.00655,0.00390,0.00470,0.00375,0.00305,0.00265,
0.00210,0.00180,0.00175,0.00235,0.00210,0.00085,0.00090,0.00095,
0.00085,0.00075,0.00040,0.00050,0.00020,0.00060,0.00030,0.00035,
0.00025,0.00025,0.00020,0.00010,0.00020,0.00025,0.00010,0.00015,
0.00005,0.00005,0.00000,0.00000,0.00000,0.00000,0.00005,0.00000,
0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,0.00000,
0.00000,0.00000,0.00000,0.00000)

ggplot()+
  geom_line(aes(x=seq(0.1,0.5,l=100),y=perc))+
  scale_x_continuous('$\\lambda$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('Percentual de falhas',labels=function(x){paste0(100*x,'%')})+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  theme_bw()
```

Veja que para $\eta>0.4$ o percentual de falha já é suficientemente baixo (menos de 1 em 10.000) e para $\eta>0.43$ não foram observadas nenhuma falha. Como a qualidade da aproximação KL não parece razoável para valores tão baixo de $\eta$, temos que esse problema de longas sequências de 0's na amostra não é relevante, uma vez que só acontece em casos onde o ajuste do modelo não será permitido.

\pagebreak

## Qualidade da compatibilização de priori: Caso Gamma

O caso Gamma é bem semelhante ao caso Poisson, uma vez que a priori conjungada para a média do modelo Gamma com parâmetro de forma ($\phi$) conhecido é Inversa-Gamma. Para fixar a notação, apresentamos a seguir o modelo usado no caso Gamma. Sejam $y$ uma v.a. tal que:

$$
\begin{aligned}
y|\phi,\mu & \sim Gamma\left(\phi,\frac{\phi}{\mu}\right)\\
\ln(\mu)& = \lambda\\
\lambda&\sim \mathcal{N}(f,q).
\end{aligned}
$$

Nesta notação, $\mu$ é a média da Gamma e $\lambda$ é o preditor linear.

Na abordagem KL, para obter a priori conjugada para $\mu$ ($\mu \sim \mathcal{IG}(\tau_1,\tau_2)$), devemos resolver o seguinte sistema:

\begin{equation}
\label{sis:4}
\begin{aligned}
\psi(\tau_1)-\ln(\tau_2) =& f\\
\frac{\tau_1}{\tau_2} =& \exp\{-f+q/2\},
\end{aligned}
\end{equation}

Usando a aproximação de segunda ordem para a função $\psi$, obtemos a seguinte solução para o sistema:

\begin{equation}
\label{sol:5}
\begin{aligned}
\tau_1 =& \frac{1}{-3+3\sqrt{1+\frac{2}{3}q}}\\
\tau_2 =& \tau_1\exp\{f-q/2\}\\,
\end{aligned}
\end{equation}

Obtidos $\tau_1$ e $\tau_2$, vamos definir $\tau^{*}_1=\tau_1+\phi$ e $\tau^{*}_2=\tau_2+\phi y$ de modo que $\mu|y \sim \mathcal{G}(\tau^{*}_1,\tau^{*}_2)$

Para obter a posteriori Normal para $\lambda$ ($\lambda|y \sim \mathcal{N}(f^{*},q^{*})$), devemos tomar $f^{*}$ e $q^{*}$ tais que:

\begin{equation}
\label{sis:5}
\begin{aligned}
f^{*} =& -\psi(\tau^{*}_1)+\ln(\tau^{*}_2)\\
q^{*} =& \psi'(\tau^{*}_1).
\end{aligned}
\end{equation}

Observe que os sistemas obtidos em (\ref{sol:5}) e (\ref{sis:5}) são quase idênticos aos obtidos em (\ref{sol:2}) e (\ref{sis:3}), respectivamente, sendo que a única diferença é o sinal associal à $f$ e $f^*$. De fato, isso se deve ao fato de que, se $X \sim \mathcal{IG}(\alpha,\beta)$, então $\frac{1}{X} \sim \mathcal{G}(\alpha,\beta)$, e se $X \sim \mathcal{LN}(\mu,\sigma^2)$, então $\frac{1}{X} \sim \mathcal{LN}(-\mu,\sigma^2)$.

Veja que a relação entre $q$ e $\tau_1$ permanece a mesma que no caso Poisson, sendo válidas todas as análises feitas à respeito (incluindo critérios para qualidade da aproximação).

A grande diferença entre a qualidade da aproximação no caso Gamma e no caso Poisson está associada a atualização dos parâmetros, uma vez que $\tau^{*}_1=\tau_1+\phi$ (ou seja, a qualidade da aproximação depende de $\phi$, o parâmetro de forma o modelo observacional, e não da média dos dados observados). De fato, para **qualquer** que sejam as observações (desde que positivas), a qualidade do ajuste (no sentido de ser próximo ao caso analítico) será a mesma, sendo determinada exclusivamente por $\phi$.

Nesta seção tentaremos encontrar o menor valor de $\phi$ tal que o ajuste é razoável. Naturalmente, para $\phi$ suficientemente grande, o ajuste será bom, porém, vale lembrar que, para $\phi$ grande, a distribuição Gamma converge à distruição Normal, portanto, estamos especialmente interessados em testar valores pequenos de $\phi$.

Adiante, exibimos a comparação entre o ajuste feito pela aproximação KL e o ajuste analítico para várias amostras i.i.d. com diversos valores de $\phi$. Em todos os casos a média utilizada foi $\mu=1$, sendo que o valor da média não é relevante para a qualidade do ajuste (i.e., a escala do dado não afeta a qualidade do ajuste).

```{r}
T=200

mu=1
p_list=list()
for(phi in c(0.35,0.5,0.6,0.75,1,2,5)){
set.seed(seed)
y=rgamma(T,phi,phi/mu)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = y,
                    family='gamma',
                    parms=list('phi'=phi))

a=resultado$conj_prior_param[1,1]
b=resultado$conj_prior_param[1,2]

a.post=a+1:T*phi
b.post=b+cumsum(y)*phi

data1=data.frame(time=1:T,
           mean=1/qgamma(0.5,a.post,b.post),
           C.I.lower=1/qgamma(0.975,a.post,b.post),
           C.I.upper=1/qgamma(0.025,a.post,b.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=1/qgamma(0.5,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.lower=1/qgamma(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=1/qgamma(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  coord_cartesian(ylim=c(0,5))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Filtragem' %>% paste(paste0('($\\phi=',phi,'$)')) %>% TeX)+
  theme_bw()

size_dist=2

mini_range=list('x'=c(150,200),
                'y'=c(0.25,1.75))
mini_place=list('x'=c(75,75+100),
                'y'=c(2,5))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(phi>=1){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[phi %>% as.character]]=p1
}
p_list$ncol=3
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

```{r}
data_plot=data.frame()
mu=1
for(phi in c(0.35,0.5,0.6,0.75,1,2,5)){
set.seed(seed)
y=rgamma(T,phi,phi/mu)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = y,
                    family='gamma',
                    parms=list('phi'=phi))

a=resultado$conj_prior_param[1,1]
b=resultado$conj_prior_param[1,2]

a.post=a+T*phi
b.post=b+sum(y)*phi

data1=data.frame(mean=1/qgamma(0.5,a.post,b.post)-mu,
           C.I.lower=1/qgamma(0.975,a.post,b.post)-mu,
           C.I.upper=1/qgamma(0.025,a.post,b.post)-mu,
           color='Sol. analítica',
           phi=phi)

data2=data.frame(mean=1/qgamma(0.5,resultado$conj_post_param[T,1],resultado$conj_post_param[T,2])-mu,
           C.I.lower=1/qgamma(0.975,resultado$conj_post_param[T,1],resultado$conj_post_param[T,2])-mu,
           C.I.upper=1/qgamma(0.025,resultado$conj_post_param[T,1],resultado$conj_post_param[T,2])-mu,
           color='Aproximação KL',
           phi=phi)

data_plot=rbind(data_plot,data1,data2)
}
```

```{r}
ggplot(data_plot)+
  geom_errorbar(aes(x=as.factor(phi),ymin=C.I.lower,ymax=C.I.upper,color=color,linetype=color))+
  geom_point(aes(x=as.factor(phi),y=mean,color=color),shape=4,size=20)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('#5555ff','black'))+
  guides(linetype=FALSE)+
  scale_x_discrete('$\\phi$' %>% TeX)+
  scale_y_continuous('Desvio de $\\mu$' %>% TeX)+
  coord_cartesian(ylim=c(-2,2))+
  theme_bw()
```

Pelos gráficos acima podemos ver que o ajuste é bem satisfatório para $\phi\ge0.75$ e parece bem decente para $0.5 \le \phi <0.75$.

De forma análoga ao caso Poisson, exibiremos ajustes em casos onde não há solução analítica (o valor de $\mu$ tem dinâmica temporal).

Amostraremos $y_1,...,y_{200}$ com as distribuições a seguir:

$$
y_i \sim Poisson\left(\phi,\frac{\phi}{\mu}\right),
$$
com:

$$
\mu=\frac{1}{10}\exp\left\{\sin\left(\frac{i}{25}\pi\right)-\frac{i}{20}\left(\frac{i}{200}-1\right)+1\right\}.
$$


Adiante exibimos o gráfico de $\mu$ ao longo do tempo junto com os valores amostrados e os ajustes para diversas escolhas de $\phi$:

```{r fig.height=4, fig.width=12}
T=200

for(phi in c(0.5,0.75,2,5)){
  set.seed(13031998)
  w <- (200 / 50) * 2 * pi
  mu=exp((sin(w * 1:T / T)))*exp(-10*(1:T/T)*((1:T/T)-1)+1)
  y=rgamma(T,phi,phi/mu)
  
  
  level=polynomial_block(order=3,values=1,D=1/1)
  season=harmonic_block(period=50,values=1,D=1/1)
  
  resultado=fit_model(level,season,
                      outcome = y,
                      family='gamma',
                      parms=list('phi'=phi))
  
  p1=(show_fit(resultado,smooth=TRUE,dynamic_plot = FALSE)$plot+
      labs(title=paste0('Ajuste ($\\phi=',phi,'$)')%>% TeX)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('#5555ff'))+
  scale_fill_manual('',values=c('#5555ff'))+guides(color=FALSE,fill=FALSE,linetype=FALSE))
  
  p2=(ggplot()+
    geom_line(aes(x=1:T,y=resultado$conj_post_param[,2]/(resultado$conj_post_param[,1]-1),color='Valor estimado'))+
    geom_ribbon(aes(x=1:T,
                    ymin=1/qgamma(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
                    ymax=1/qgamma(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
                    color='Valor estimado'),alpha=0,linetype='dashed')+
    scale_color_manual('',values=c('#5555ff','black'))+
    labs(title=paste0('Filtragem ($\\phi=',phi,'$)')%>% TeX)+
      scale_y_continuous('')+
    coord_cartesian(ylim=c(0,100))+
    geom_line(aes(x=1:T,y=mu,color='Valor verdadeiro'))+
    theme_bw()+guides(color=FALSE,fill=FALSE,linetype=FALSE))
  
    alpha=rep(NA,T)
    beta=rep(NA,T)
    for(i in 1:T){
      ft=t(resultado$FF[,,i]) %*% resultado$mts[,i]
      Qt=t(resultado$FF[,,i]) %*% resultado$Cts[,,i] %*% resultado$FF[,,i]
    
      alpha[i]=1/(-3+3*sqrt(1+2*Qt[1,1]/3))
      beta[i]=alpha[i]*exp(ft[1]-Qt[1,1]/2)
    }
    
    p3=(ggplot()+
      geom_line(aes(x=1:T,y=beta/(alpha-1),color='Est.'))+
      geom_ribbon(aes(x=1:T,
                      ymin=1/qgamma(0.975,alpha,beta),
                      ymax=1/qgamma(0.025,alpha,beta),
                      color='Est.'),alpha=0,linetype='dashed')+
      scale_color_manual('',values=c('#5555ff','black'))+
      scale_y_continuous('')+
      labs(title=paste0('Suavizacao ($\\phi=',phi,'$)')%>% TeX)+
      coord_cartesian(ylim=c(0,100))+
      geom_line(aes(x=1:T,y=mu,color='phi'))+
      theme_bw())
    
  grid.arrange(p1, p2,p3, ncol=3)
}
```


\pagebreak

## Qualidade da compatibilização de priori: Caso Multinomial

Nesta seção trataremos da qualidade do ajuste para o caso Multinomial. Para a análise deste caso, nos limitaremos ao caso Binomial, pois este caso já é o suficiente para determinar a qualidade da aproximação, desta forma trataremos do seguinte modelo:

$$
\begin{aligned}
y|\pi& \sim Binom\left(m,\pi\right)\\
\ln\left(\frac{\pi}{1-\pi}\right)& = \lambda\\
\lambda&\sim \mathcal{N}(f,q).
\end{aligned}
$$

Nesta notação, $\pi$ é a probabilidade de sucesso, $m$ é o número de ensaios e $\lambda$ é o preditor linear.

Na abordagem KL, para obter a priori conjugada para $\pi$ ($\pi \sim \mathcal{B}(\tau_1,\tau_2)$), devemos resolver o seguinte sistema:

\begin{equation}
\label{sis:6}
\begin{aligned}
\psi(\tau_1)-\psi(\tau_1+\tau_2) =& f\\
\psi(\tau_2)-\psi(\tau_1+\tau_2) =& -\ln\left(1+e^f\right)+\frac{q}{2}e^f(1+e^f),
\end{aligned}
\end{equation}

Para obter a solução para este sistema é necessário recorrer ao método Newton-Raphson (N-R). Durante a última reunião, mencionei que havia um problema no ajuste quando $\tau_1+\tau_2$ é muito grande, de fato, parecia haver uma convergência da solução de modo a forçar $\tau_1+\tau_2<400$, porém, durante a produção deste relatório, identifiquei que o problema era a tolerância da solução do N-R. Para uma estimação adequada dos parâmetros é necessário que tolerância seja bem baixa ($1e-20$ funciounou bem), mas o \emph{default} do algoritmo no $R$ é relativamente alto ($1e-6$).

Obtidos $\tau_1$ e $\tau_2$, definimos $\tau^{*}_1=\tau_1+y$ e $\tau^{*}_2=\tau_2+m-y$ de modo que $\pi|y \sim \mathcal{B}(\tau^{*}_1,\tau^{*}_2)$

Para obter a posteriori Normal para $\lambda$ ($\lambda|y \sim \mathcal{N}(f^{*},q^{*})$), devemos tomar $f^{*}$ e $q^{*}$ tais que:

\begin{equation}
\label{sis:7}
\begin{aligned}
f^{*} =& \psi(\tau^*_1)-\psi(\tau^*_2)\\
q^{*} =& \psi'(\tau^{*}_1)+\psi'(\tau^{*}_2).
\end{aligned}
\end{equation}

Para a análise da qualidade da aproximação, estamos interessados em descobrir quais os valores de $m$ e $\pi$ para os quais o ajuste fica satisfatório. Vale destacar que, por simetria, basta testar valores de $\pi\le0.5$.

O primeiro teste realizado é semelhante ao primeiro teste do caso Poisson: Começamos com uma distribuição Beta, aproximamos usando o método KL e então trazemos de volta para distribuição Beta usando o sistema de volta. Para cada teste, escolhemos $p\in (0,0.5]$ e $n \in [0,300]$, e realizamos o teste para uma Beta com parâmetros $\tau_1=pn$ e $\tau_2=(1-p)n$. É importante destacar que, neste caso, $p$ **não** é a probabilidade de sucesso (não confundir com $\pi$) e $n$ **não** o número de ensaios (não confundir com $m$)! Estamos apenas reparametrizando a distribuição Beta para facilitar a análise, ainda não estamos gerando dados ou ajustando modelos. Podemos pensar em $p$ como algo que representa a simetria ou assimetria da Beta e $n$ como algo que representa a concentração da Beta. O intuito dos próximo gráficos é avaliar a qualidade da aproximação para distribuições Beta com precisão baixa e/ou bastante assimétricas. 

```{r include=FALSE}

data_plot=data.frame(time=NULL)
# p=0.5
for(p in c(0.005,0.01,0.025,0.05,0.1,0.25,0.3,0.4,0.5)){
  inter_x1=function(x){do.call(convert_Dir_Normal,convert_Normal_Dir(c(p*x,(1-p)*x)))[1]}
  inter_x2=function(x){do.call(convert_Dir_Normal,convert_Normal_Dir(c(p*x,(1-p)*x)))[2]}
  x=seq(0,150,l=101)[-1]
  fx1=rep(NA,100)
  fx2=rep(NA,100)
  for(i in 1:100){
    fx1[i]=inter_x1(x[i])
    fx2[i]=inter_x2(x[i])
  }
  data_plot=rbind(data_plot,data.frame(x=x,fx1=fx1,fx2=fx2,p=p,label=p))
}
data_plot$label=factor(data_plot$label,levels=c(0.005,0.01,0.025,0.05,0.1,0.25,0.3,0.4,0.5))
```


```{r}
p1=ggplot(data_plot)+
    geom_line(aes(x=x,y=fx1,color='Sucesso',linetype='Aprox.'))+
    geom_line(aes(x=x,y=fx2,color='Fracasso',linetype='Aprox.'))+
    geom_line(aes(x=x,y=p*x,color='Sucesso',linetype='Val. exato'))+
    geom_line(aes(x=x,y=(1-p)*x,color='Fracasso',linetype='Val. exato'))+
    geom_vline(xintercept = 0,linetype='dashed')+
    geom_hline(yintercept = 0,linetype='dashed')+
    scale_x_continuous('n' %>% TeX)+
    scale_y_continuous('\tau_i' %>% TeX)+
    scale_color_manual('',values=c('#dd5555','#55dd55'))+
    coord_cartesian(ylim=c(0,150),expand=c(0,0))+
    theme_bw()+
  facet_wrap(~paste0('p=',label),scale='free',ncol=3)
p1
```


Veja que, para $p$ razoavelmente grande (0.1) a aproximação parece conservar bem os parâmetro e, mesmo quando $p$ é bem pequeno, basta que $n$ seja adequadamente grande para que a aproximação fique boa.

Uma característica muito importante da aproximação KL é que há um certo deslocamento dos parâmetros após a conversão, como pode ser visto a seguir:

```{r}
p1=ggplot(data_plot[data_plot$label==0.5,])+
    geom_line(aes(x=x,y=fx1,color='Sucesso',linetype='Aprox.'))+
    geom_line(aes(x=x,y=fx2,color='Fracasso',linetype='Aprox.'))+
    geom_line(aes(x=x,y=p*x,color='Sucesso',linetype='Val. exato'))+
    geom_line(aes(x=x,y=(1-p)*x,color='Fracasso',linetype='Val. exato'))+
    geom_vline(xintercept = 0,linetype='dashed')+
    geom_hline(yintercept = 0,linetype='dashed')+
    scale_x_continuous('n' %>% TeX)+
    scale_y_continuous('$\\tau_i$' %>% TeX)+
    scale_color_manual('',values=c('#dd5555','#55dd55'))+
    labs(title='p=0.5')+
    coord_cartesian(xlim=c(0,100),ylim=c(0,50),expand=c(0,0))+
    theme_bw()
size_dist=2

mini_range=list('x'=c(70,75),
                'y'=c(70,75)/2)
mini_place=list('x'=c(10,50)+40,
                'y'=c(10,50)/2)

p2=p1+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
    labs(title='')+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
(p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = mini_range$x[1],
                xmax = mini_range$x[2],
                ymin = mini_range$y[1],
                ymax = mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = mini_place$x[1],
                xmax = mini_place$x[2],
                ymin = mini_place$y[1],
                ymax = mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])  ) %>% print
```

O descolamento da curva é em torno de $0.5$ para baixo em relação ao valor exato, e é o mesmo para qualquer $p$ desde que o $n$ seja suficientemente grande (de fato esse deslocamento converge assintoticamente por cima para 0.5 conforme $n$ aumenta). A princípio pode não parecer relevante esse deslocamente, afinal, 0.5 é pouco para $n$ suficientemente grande, porém, veremos mais a frente que esse valor pode ser bem problemático para alguns valores de $m$, especificamente no caso Bernoulli onde $m=1$. 

Avaliaremos agora a qualidade do ajuste olhando para a densidade da Beta obtida após a conversão:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

p=0.01
for(N in c(10,20,30,40)){
  new_norm=do.call(convert_Dir_Normal,convert_Normal_Dir(c(p*N,(1-p)*N)))
  alpha1_alt=new_norm[1]
  alpha2_alt=new_norm[2]
  
  
  x=seq(0,0.1,l=100)
  
  fx_true=dbeta(x,p*N,(1-p)*N)
  fx_approx=dbeta(x,alpha1_alt,alpha2_alt)
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             N=paste0('$\\n=',N,'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx,
                             N=paste0('$\\n=',N,'$'),
                             color='Distr. aprox.'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
   labs(title=paste0('$p=',p,'$') %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(N,levels=unique(data_plot$N)),nrow=3))
```

podemos observar que, para $p=0.01$, a aproximação começa a ficar boa por volta do $n=20$, ou seja, com $\tau_1=0.2$.

```{r fig.height=12}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

p=0.1
for(N in c(1,2,5,10,15,20)){
  new_norm=do.call(convert_Dir_Normal,convert_Normal_Dir(c(p*N,(1-p)*N)))
  alpha1_alt=new_norm[1]
  alpha2_alt=new_norm[2]
  
  
  x=seq(0,0.4,l=100)
  
  fx_true=dbeta(x,p*N,(1-p)*N)
  fx_approx=dbeta(x,alpha1_alt,alpha2_alt)
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             N=paste0('$\\n=',N,'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx,
                             N=paste0('$\\n=',N,'$'),
                             color='Distr. aprox.'))
}
```


```{r fig.height=12}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
   labs(title=paste0('$p=',p,'$') %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(N,levels=unique(data_plot$N)),nrow=3))
```

No gráfico acima podemos ver que, para $p=0.1$, a aproximação começa a ficar razoável já para valores tão baixo quanto $n=2$, sendo muito boa já para $n=5$. Novamente, temos uma boa aproximação para valores de $\tau_1$ tão baixos quanto $0.5$ e $0.2$.

```{r, include=FALSE}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

p=0.25
for(N in c(0.1,0.5,1,2,5,10,15,20)){
  new_norm=do.call(convert_Dir_Normal,convert_Normal_Dir(c(p*N,(1-p)*N)))
  alpha1_alt=new_norm[1]
  alpha2_alt=new_norm[2]
  
  
  x=seq(0,1,l=100)
  
  fx_true=dbeta(x,p*N,(1-p)*N)
  fx_approx=dbeta(x,alpha1_alt,alpha2_alt)
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             N=paste0('$\\N=',N,'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx,
                             N=paste0('$\\N=',N,'$'),
                             color='Distr. aprox.'))
}
```


```{r fig.height=16}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
   labs(title=paste0('$p=',p,'$') %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(N,levels=unique(data_plot$N)),nrow=4))
```

Para $p=0.25$ temos que a qualidade da aproximação fica boa já a partir de $n=1$, novamente, temos que a aproximação começa a ficar boa com $\tau_1\approx 0.2$.

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

p=0.5
for(N in c(0.1,0.5,1,2,5,10,15,20)){
  new_norm=do.call(convert_Dir_Normal,convert_Normal_Dir(c(p*N,(1-p)*N)))
  alpha1_alt=new_norm[1]
  alpha2_alt=new_norm[2]
  
  
  x=seq(0,1,l=100)
  
  fx_true=dbeta(x,p*N,(1-p)*N)
  fx_approx=dbeta(x,alpha1_alt,alpha2_alt)
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             N=paste0('$\\n=',N,'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx,
                             N=paste0('$\\n=',N,'$'),
                             color='Distr. aprox.'))
}
```


```{r fig.height=16}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
   labs(title=paste0('$p=',p,'$') %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(N,levels=unique(data_plot$N)),nrow=4))
```

Pelo gráfico acima, podemos observar uma certa mudança de comportamente na qualidade da aproximação, que é razoável para $n=0.5$ e $n=1$, mas não parece tão boa para $n=1$. Possivelmente a troca de concavidade da densidade pode ser responsável por isso, em especial, pode ser que a aproximação de segunda ordem usada para montar o sistema seja ruim próximo para certos valores de $f$ onde há a mudança de concavidade na densidade. Mais análises são necessárias para verificar a extensão desse fenômeno e se ele de fato causa algum prejuizo. Dito isso, observe que, após uma iteração do processo de filtragem, $\tau_1+\tau_2 > 1$, pois $m\ge 1$, logo, mesmo se esse caso for problemático, ele só pode causar problemas nas iterações iniciais.

Avaliaremos agora a qualidade do ajuste comparando os parâmetros estimados pela aproximação KL e os valores obtidos analíticamente para uma amostra i.i.d. de uma Binomial com os parâmetros adequados. Nesta etapa vamos avaliar a qualidade do ajuste para diferentes observações (anteriormente estavamos avaliando a qualidade da aproximação para diferentes prioris). A ideia destes testes é encontrar quais os valores mínimos de $m$ e $\pi$ para os quais o modelo se ajusta bem (anteriormente estávamos avaliando quais os valores mínimos de $n$ e $p$).

Caso $m=1$ (Bernoulli):

```{r fig.width=8}
T=200

N=1
p_list=list()
for(p in c(0.01,0.1,0.25,0.5)){
set.seed(seed)
y=rbinom(T,N,p)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = cbind(y,N-y),
                    family='multinomial')

alpha1=resultado$conj_prior_param[1,1]
alpha2=resultado$conj_prior_param[1,2]

alpha1.post=alpha1+cumsum(y)
alpha2.post=alpha2+cumsum(N-y)

data1=data.frame(time=1:T,
           mean=alpha1.post/(alpha1.post+alpha2.post),
           C.I.lower=qbeta(0.025,alpha1.post,alpha2.post),
           C.I.upper=qbeta(0.975,alpha1.post,alpha2.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1]/(resultado$conj_post_param[,1]+resultado$conj_post_param[,2]),
           C.I.lower=qbeta(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=qbeta(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  scale_linetype_manual('',values=c('dotdash','solid'))+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
  coord_cartesian(ylim=c(0,1))+
  labs(title='Filtragem Bernoulli' %>% paste(paste0('($\\pi=',p,'$)')) %>% TeX)+
  theme_bw()

size_dist=0.1*max(data_plot$C.I.upper)

mini_range=list('x'=c(150,200),
                'y'=c(data2$C.I.lower[T]-size_dist,data2$C.I.upper[T]+size_dist))
mini_place=list('x'=c(75,75+100),
                'y'=c(0.5,0.5+2*(data2$C.I.upper[T]-data2$C.I.lower[T]+2*size_dist)))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(p<0.1){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[p %>% as.character]]=p1
}
p_list$ncol=2
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Pelos gráficos acima podemos verificar que o ajuste para o caso Bernoulli fica significativamente diferente da solução analítica, mesmo para $\pi=0.5$. A causa disso é o deslocamento observado nos parâmetros após a conversão da priori e da posteriori. Como mencionado anteriormente, quando a priori tem uma precisão minimamente razoável há um \emph{shift} de 0.5 no valor $\tau_1+\tau_2$ da distribuição conjugada. Veja que, no caso Bernoulli, $\tau_1+\tau_2$ deveria aumentar em uma unidade a cada iteração, mas, por causa do \emph{shift}, esse aumento acaba sendo de $0.5$, assim, a solução pela aproximação KL tem o valor $\tau_1+\tau_2$ igual a metade do ideal.

Caso $m=2$:

```{r fig.width=8}
T=200

N=2
p_list=list()
for(p in c(0.01,0.1,0.25,0.5)){
set.seed(seed)
y=rbinom(T,N,p)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = cbind(y,N-y),
                    family='multinomial')

alpha1=resultado$conj_prior_param[1,1]
alpha2=resultado$conj_prior_param[1,2]

alpha1.post=alpha1+cumsum(y)
alpha2.post=alpha2+cumsum(N-y)

data1=data.frame(time=1:T,
           mean=alpha1.post/(alpha1.post+alpha2.post),
           C.I.lower=qbeta(0.025,alpha1.post,alpha2.post),
           C.I.upper=qbeta(0.975,alpha1.post,alpha2.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1]/(resultado$conj_post_param[,1]+resultado$conj_post_param[,2]),
           C.I.lower=qbeta(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=qbeta(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  scale_linetype_manual('',values=c('dotdash','solid'))+
    guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  coord_cartesian(ylim=c(0,1))+
  labs(title='Filtragem m=2' %>% paste(paste0('($\\pi=',p,'$)')) %>% TeX)+
  theme_bw()

size_dist=0.05*max(data_plot$C.I.upper)

mini_range=list('x'=c(150,200),
                'y'=c(data2$C.I.lower[T]-size_dist,data2$C.I.upper[T]+size_dist))
mini_place=list('x'=c(75,75+100),
                'y'=c(0.6,0.6+2*(data2$C.I.upper[T]-data2$C.I.lower[T]+2*size_dist)))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(p<0.1 | N>1){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[p %>% as.character]]=p1
}
p_list$ncol=2
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Podemos ver pelos gráficos acima que o \emph{shift} dos parâmetros na conversão da priori tem um impacto muito menor quando $m=2$, de fato, os ajustes parecem satisfatórios para $p\ge 0.1$. Vale destacar que, neste modelo, $m \in \mathbb{Z}_+$, logo a menos do caso Bernoulli, todos os outros outros ajuste serão pelo menos tão bons quanto este, uma vez que o caso Bernoulli é o único onde $m<2$.

Caso $m=5$:

```{r fig.width=8}
T=200

N=5
p_list=list()
for(p in c(0.01,0.1,0.25,0.5)){
set.seed(seed)
y=rbinom(T,N,p)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = cbind(y,N-y),
                    family='multinomial')

alpha1=resultado$conj_prior_param[1,1]
alpha2=resultado$conj_prior_param[1,2]

alpha1.post=alpha1+cumsum(y)
alpha2.post=alpha2+cumsum(N-y)

data1=data.frame(time=1:T,
           mean=alpha1.post/(alpha1.post+alpha2.post),
           C.I.lower=qbeta(0.025,alpha1.post,alpha2.post),
           C.I.upper=qbeta(0.975,alpha1.post,alpha2.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1]/(resultado$conj_post_param[,1]+resultado$conj_post_param[,2]),
           C.I.lower=qbeta(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=qbeta(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  scale_linetype_manual('',values=c('dotdash','solid'))+
    guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  coord_cartesian(ylim=c(0,1))+
  labs(title='Filtragem m=5' %>% paste(paste0('($\\pi=',p,'$)')) %>% TeX)+
  theme_bw()

size_dist=0.1*max(data_plot$C.I.upper)

mini_range=list('x'=c(150,200),
                'y'=c(data2$C.I.lower[T]-size_dist,data2$C.I.upper[T]+size_dist))
mini_place=list('x'=c(75,75+100),
                'y'=c(0.6,0.6+2*(data2$C.I.upper[T]-data2$C.I.lower[T]+2*size_dist)))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(p<0.1 | N>1){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[p %>% as.character]]=p1
}
p_list$ncol=2
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Caso $m=10$:

```{r fig.width=8}
T=200

N=10
p_list=list()
for(p in c(0.01,0.1,0.25,0.5)){
set.seed(seed)
y=rbinom(T,N,p)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = cbind(y,N-y),
                    family='multinomial')

alpha1=resultado$conj_prior_param[1,1]
alpha2=resultado$conj_prior_param[1,2]

alpha1.post=alpha1+cumsum(y)
alpha2.post=alpha2+cumsum(N-y)

data1=data.frame(time=1:T,
           mean=alpha1.post/(alpha1.post+alpha2.post),
           C.I.lower=qbeta(0.025,alpha1.post,alpha2.post),
           C.I.upper=qbeta(0.975,alpha1.post,alpha2.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1]/(resultado$conj_post_param[,1]+resultado$conj_post_param[,2]),
           C.I.lower=qbeta(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=qbeta(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
    guides(color=FALSE,linetype=FALSE)+
  scale_linetype_manual('',values=c('dotdash','solid'))+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  coord_cartesian(ylim=c(0,1))+
  labs(title='Filtragem m=10' %>% paste(paste0('($p=',p,'$)')) %>% TeX)+
  theme_bw()

size_dist=0.1*max(data_plot$C.I.upper)

mini_range=list('x'=c(150,200),
                'y'=c(data2$C.I.lower[T]-size_dist,data2$C.I.upper[T]+size_dist))
mini_place=list('x'=c(75,75+100),
                'y'=c(0.6,0.6+2*(data2$C.I.upper[T]-data2$C.I.lower[T]+2*size_dist)))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(p<0.1 | N>1){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[p %>% as.character]]=p1
}
p_list$ncol=2
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Caso $m=20$:

```{r fig.width=8}
T=200

N=20
p_list=list()
for(p in c(0.01,0.1,0.25,0.5)){
set.seed(seed)
y=rbinom(T,N,p)

level=polynomial_block(order=1,values=1,D=1/1)

resultado=fit_model(level,
                    outcome = cbind(y,N-y),
                    family='multinomial')

alpha1=resultado$conj_prior_param[1,1]
alpha2=resultado$conj_prior_param[1,2]

alpha1.post=alpha1+cumsum(y)
alpha2.post=alpha2+cumsum(N-y)

data1=data.frame(time=1:T,
           mean=alpha1.post/(alpha1.post+alpha2.post),
           C.I.lower=qbeta(0.025,alpha1.post,alpha2.post),
           C.I.upper=qbeta(0.975,alpha1.post,alpha2.post),
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1]/(resultado$conj_post_param[,1]+resultado$conj_post_param[,2]),
           C.I.lower=qbeta(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           C.I.upper=qbeta(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  scale_linetype_manual('',values=c('dotdash','solid'))+
    guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  coord_cartesian(ylim=c(0,1))+
  labs(title='Filtragem m=20' %>% paste(paste0('($p=',p,'$)')) %>% TeX)+
  theme_bw()

size_dist=0.1*max(data_plot$C.I.upper)

mini_range=list('x'=c(150,200),
                'y'=c(data2$C.I.lower[T]-size_dist,data2$C.I.upper[T]+size_dist))
mini_place=list('x'=c(75,75+100),
                'y'=c(0.6,0.6+2*(data2$C.I.upper[T]-data2$C.I.lower[T]+2*size_dist)))

p2=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
    coord_cartesian(ylim=c(mini_range$y[1],mini_range$y[2]),xlim=c(mini_range$x[1],mini_range$x[2]))+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    guides(color=FALSE,linetype=FALSE)+
    scale_x_continuous('',expand=c(0,0))+
    scale_y_continuous('',expand=c(0,0))+
    theme_bw()+
  theme(axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks=element_blank(),
        panel.border=element_blank(),
        plot.margin=unit(rep(0,4), "cm"))
if(p<0.1 | N>1){
p1=p1+
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[1],
                                  mini_place$y[1],
                                  mini_range$y[1],
                                  mini_place$y[1])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_path(aes(x,y,group=grp), 
            data=data.frame(x = c(mini_range$x[1],
                                  mini_place$x[1],
                                  mini_range$x[2],
                                  mini_place$x[2]),
                            y=c(mini_range$y[2],
                                  mini_place$y[2],
                                  mini_range$y[2],
                                  mini_place$y[2])
                            ,grp=c(1,1,2,2)),
            linetype='dashed') +
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_range$x[1],x2=mini_range$x[2],y1=mini_range$y[1],y2=mini_range$y[2]), color='black', linetype='solid', alpha=0)+
  geom_rect(aes(xmin = x1,
                xmax = x2,
                ymin = y1,
                ymax = y2),data=data.frame(x1=mini_place$x[1],x2=mini_place$x[2],y1=mini_place$y[1],y2=mini_place$y[2]), color='black', fill='white', linetype='solid', alpha=1,size=line_width) +
  annotation_custom(ggplotGrob(p2),
                    xmin = mini_place$x[1],
                    xmax = mini_place$x[2],
                    ymin = mini_place$y[1], 
                    ymax = mini_place$y[2])
}
p_list[[p %>% as.character]]=p1
}
p_list$ncol=2
do.call(grid.arrange,p_list)
```

As curvas em preto representam o ajuste analítico em cada caso e a curva em azul representa o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Neste momento, é relevante discutir a extensão deste caso para a Multinomial com mais de 2 categorias. Como observado nas análises da qualidade da aproximação, temos um resultado satisfatório desde que $\tau_1\ge0.2$. Esse resultado se extende para o caso Multinomial geral, ão mostrei essa análise neste relatório para não ficar muito extenso, mas pude verificar que este é o caso, ademais, parece razoavelmente intuitivo dado a forma do sistema que um resultado assim aconteceria. Pelos gráficos acima, a menos do caso Bernoulli, o ajuste fica satisfatório desde que $\pi m \ge 0.2$ e esse resultado se extende para o caso de 3 ou mais categorias. De modo geral, uma eurísitca para determinar se a aproximação das distribuições será satisfatória é avaliar se o valor esperado de ocorrências é maior ou igual a 0.2 em todas as categorias que se deseja modelar, se esta condição for satisfeita, então o ajuste ficará bom para todas as categorias. Caso uma ou mais categorias falhem em satisfazer essa condição, não há garantias de que o ajuste ficará bom. Nos casos em que observei, o não cumprimento da condição gerou superestimação da incerteza **apenas** nas categorias onde a condição não vale, porém, mais análises são necessárias para verificar se este é sempre o caso.

Para finalizar esta seção, apresento o ajuste de 3 casos onde não há solução analítica. Em cada caso vamos usar um valor distinto de $m$ (5,2,1) e a probabilidade de sucesso terá dinâmica temporal, de modo que:

$$
\begin{aligned}
  \lambda_i=& \frac{1}{10} \exp\left\{\sin\left(\frac{i\pi}{25}\right)-\left(\frac{i}{20}\right)\left(\frac{i}{200}-1\right)\right\}\\
  \pi_i=&\frac{\lambda_i}{1+\lambda_i}
\end{aligned}
$$


```{r fig.height=4}
T=200

for(N in c(5,2,1)){
  set.seed(13031998)
  w <- (200 / 50) * 2 * pi
  eta=exp((sin(w * 1:T / T)))*exp(-10*(1:T/T)*((1:T/T)-1))/10
  p=eta/(1+eta)
  y=rbinom(T,N,p)
  
  
  level=polynomial_block(order=3,values=1,D=1/1)
  season=harmonic_block(period=50,values=1,D=1/1)
  
  resultado=fit_model(level,season,
                      outcome = cbind(y,N-y),
                      family='multinomial')
  # show_fit(resultado,smooth = TRUE, dynamic_plot=FALSE)$plot %>% print
  
  p1=(ggplot()+
    geom_line(aes(x=1:T,y=resultado$conj_post_param[,1]/(resultado$conj_post_param[,1]+resultado$conj_post_param[,2]),color='Valor estimado'))+
    geom_ribbon(aes(x=1:T,
                    ymin=qbeta(0.025,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
                    ymax=qbeta(0.975,resultado$conj_post_param[,1],resultado$conj_post_param[,2]),
                    color='Valor estimado'),alpha=0,linetype='dashed')+
    scale_color_manual('',values=c('#5555ff','black'))+
      scale_y_continuous('$\\pi$' %>% TeX)+
        guides(color=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    labs(title=paste0('Filtragem ($N=',N,'$)')%>% TeX)+
      coord_cartesian(ylim=c(0,1))+
    geom_line(aes(x=1:T,y=p,color='Valor verdadeiro'))+
    theme_bw())
  
    alpha=rep(NA,T)
    beta=rep(NA,T)
    for(i in 1:T){
      ft=t(resultado$FF[,,i]) %*% resultado$mts[,i]
      Qt=t(resultado$FF[,,i]) %*% resultado$Cts[,,i] %*% resultado$FF[,,i]
    
      alpha[i]=convert_Dir_Normal(ft,Qt)[1]
      beta[i]=convert_Dir_Normal(ft,Qt)[2]
    }
    
    p2=(ggplot()+
      geom_line(aes(x=1:T,y=alpha/(alpha+beta),color='Valor estimado'))+
      geom_ribbon(aes(x=1:T,
                      ymin=qbeta(0.025,alpha,beta),
                      ymax=qbeta(0.975,alpha,beta),
                      color='Valor estimado'),alpha=0,linetype='dashed')+
      scale_color_manual('',values=c('#5555ff','black'))+
        guides(color=FALSE)+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
      labs(title=paste0('Suavizacao ($N=',N,'$)')%>% TeX)+
      geom_line(aes(x=1:T,y=p,color='Valor verdadeiro'))+
        scale_y_continuous('$\\pi$' %>% TeX)+
      coord_cartesian(ylim=c(0,1))+
      theme_bw())
    grid.arrange(p1,p2,ncol=2)
}
```

As curvas em preto representam o valor real do parâmetro em cada caso e as curvas em azul representam o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.


\pagebreak

## Qualidade da compatibilização de priori: Caso Normal com variância desconhecida

Para finalizar este relatório, resta apenas lidar com o caso Normal com variância desconhecida. Este é, de fato, o caso com a pior qualidade de aproximação, ainda assim sob certas condições, o ajuste gerado me pareceu razoável. Entraremos nos detalhes sobre as condições para que o ajuste dos dados seja minimamente adequado, mas antes devemos apresentar o modelo utilizado e os sistemas a serem resolvidos afim de contextualizar o problema e fixar a notação:

$$
\begin{aligned}
y|\mu, \phi & \sim Normal\left(\mu,\phi\right)\\
\mu& = \lambda_1\\
\ln(\phi)& = \lambda_2\\
\lambda_1,\lambda_2&\sim \mathcal{N}_2(f_1,f_2,q_1,q_2,\rho).
\end{aligned}
$$

Nesta notação, $\mu$ é a média dos dados, $\phi$ é a precisão, $\mathcal{N}_2(f_1,f_2,q_1,q_2,\rho)$ representa a distribuição Normal bivariada com médias $f_1$ e $f_2$, variâncias $q_1$ e $q_2$ e correlação $\rho$ (chamaremos também $q_{12}=\rho\sqrt{q_1 q_2}$) e $\lambda=(\lambda_1,\lambda_2)'$ é o preditor linear.

Na abordagem KL, para obter a priori conjugada para $\mu,\phi$ ($\mu,\phi \sim \mathcal{NG}(\mu_0,c_0,\alpha,\beta)$), devemos resolver o seguinte sistema:

\begin{equation}
\label{sis:NG0}
\begin{aligned}
\mathbb{E}_\mathcal{N}[H_\mathcal{NG}]=\mathbb{E}_\mathcal{NG}[H_\mathcal{NG}],
\end{aligned}
\end{equation}

onde $H_\mathcal{NG}$ é o vetor de estatísticas suficientes da distribuição Normal-Gamma, i.e.:

$$
H_\mathcal{NG}=(\ln(\phi),\phi,\phi \mu^2,\phi \mu)'.
$$

Resolvendo o sistema (\ref{sis:NG0}) obtemos os seguintes parâmetros para a priori conjugada:

\begin{equation}
\label{sis:NG1}
\begin{aligned}
\mu_0 =& f1+q_{12}\\
c_0 =& \frac{1}{q_1}\exp\{-f_2-q_2/2\}\\
\alpha =& \frac{1}{-3+3\sqrt{1+\frac{2}{3}q_2}}\\
\beta =& \alpha\exp\{-f_2-q_2/2\},
\end{aligned}
\end{equation}

Obtidos os parâmetros da priori conjugada, podemos obter os parâmetros da posteriori conjugada a partir das seguintes equações:

\begin{equation}
\label{sis:NG3}
\begin{aligned}
  \mu^{*}_0  =& \frac{c_0\mu_0 + y}{c_0 + 1}\\
  c^{*}_0    =& c_0 + 1\\
  \alpha^{*} =& \alpha + 0.5\\
  \beta^{*}  =& \beta + \frac{1}{2}c_0\frac{(\mu_0 - y)^2}{c_0 + 1}
\end{aligned}
\end{equation}

Para obter a posteriori Normal para $\lambda$ ($\lambda|y \sim \mathcal{N}_2(f_1^{*},f_2^{*},q_1^{*},q_2^{*},\rho^{*})$), precisamos resolver o sistema a seguir:

\begin{equation}
\label{sis:NG2}
\begin{aligned}
\mathbb{E}_\mathcal{N}[H_\mathcal{N}]=\mathbb{E}_\mathcal{NG}[H_\mathcal{N}],
\end{aligned}
\end{equation}

onde $H_\mathcal{N}$ é o vetor de estatísticas suficientes da distribuição Normal bivariada, i.e.:

$$
H_\mathcal{N}=(\lambda_1,\lambda_2,\lambda_1^2,\lambda_2^2,\lambda_1\lambda_2)'.
$$

Resolvendo o sistema (\ref{sis:NG2}) obtemos os seguintes parâmetros para a posteriori Normal::

\begin{equation}
\label{sis:NG4}
\begin{aligned}
  f_1^{*}  =& \mu^{*}_0\\
  f_2^{*}   =& \psi(\alpha^{*})-\ln(\beta^{*})\\
  q_1^{*} =& \frac{\beta^{*}}{c_0^{*}(\alpha^{*}-1)}\\
  q_2^{*}  =& \psi'(\alpha^{*})\\
  \rho^{*} =& 0
\end{aligned}
\end{equation}

No restante desta seção estaremos interessado em encontrar as condições para que a aproximção KL seja satisfatória. Neste sentido, a primeira coisa a se observar é que a qualidade da aproximação **não** depende da escala ou locação dos dados. De fato, se tomamos $y^*=ay+b$, então a priori normal para o novo preditor linear será $\mathcal{N}_2(af_1+b,f_2-2ln(a),a^2 q_1,q_2,\rho)$, daí a priori conjugada será $\mathcal{NG}(a\mu_0+b,c_0,\alpha,a^2\beta)$, a posteriori conjugada será $\mathcal{NG}(a\mu_0^*+b,c_0^*,\alpha^*,a^2\beta^*)$ e a posteriori normal será $\mathcal{N}_2(af_1^*+b,f_2^*-2ln(a),a^2 q_1^*,q_2^*,\rho^*)$, ou seja, mudar a escala dos dados e fazer o ajuste equivale a fazer o ajuste e mudar a escala dos dados. Com isto em mente, podemos fazer as análises sem nos preocupar com a escala ou locação dos dados.

De forma semelhante ao que foi feito no caso Poisson, a primeira coisa a ser verificada é se a aplicação da conversão de distribuições causa alguma degeneração nos parâemtros, especificamente, vamos avaliar o que ocorre com os parâmetros da Normal após a compatibilização quando não atualizamos a distribuição de $\mu,\phi$, isto é, se começamos com uma distribuição para $\lambda_1,\lambda_2$, obtemos a distribuição conjugada para $\mu,\phi$ e depois obtemos a distribuição $\lambda_1^*,\lambda_2^*$ sem atualizar a distribuição de $\mu,\phi$, o quão diferente é a distribuição de  $\lambda_1,\lambda_2$ e $\lambda_1^*,\lambda_2^*$?

Para responder esta pergunta podemos observar que:
$$
\begin{aligned}
  f_1^{*}  =& \mu^{*}_0 = \mu_0=f_1+q_{12}\\
  f_2^{*}   =& \psi(\alpha^{*})-\ln(\beta^{*}) =  \psi(\alpha)-\ln(\beta) = f_2\\
  q_1^{*} =& \frac{\beta^{*}}{c_0^{*}(\alpha^{*}-1)} = \frac{\beta}{c_0(\alpha-1)} = \frac{\alpha}{\alpha-1}q_1\\
  =& \frac{\frac{1}{-3+3\sqrt{1+\frac{2}{3}q_2}}}{\frac{1}{-3+3\sqrt{1+\frac{2}{3}q_2}}-1}q_1 \\
  q_2^{*}  =& \psi'(\alpha^{*})= \psi'(\alpha) =\psi'\left( \frac{1}{-3+3\sqrt{1+\frac{2}{3}q_2}}\right)\\
  \rho^{*} =& 0
\end{aligned}
$$
O comportamento de $q_2^*$ é análogo ao comportamento descrito para a variância do preditor linear no caso Poisson. O efeito da compatibilização nas médias não é problemático, então resta avaliar a distorção em $q_1$. Com mencionado anteriormente, a escala do dado não é relevante, portante, vamos tratar do caso onde $q_1=1$ e avaliar o valor de $q_1^*$ para vários valores de $q_2$:


```{r}
inter_x1=function(x){(1/(-3+3*sqrt(1+2*x/3)))/(1/(-3+3*sqrt(1+2*x/3))-1)}
x=seq(0,7/6,l=1000)

p1=ggplot()+
    geom_line(aes(x=x,y=inter_x1(x),color='Aproximação KL'))+
    geom_line(aes(x=x,y=1,color='Ideal'),linetype='dashed')+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#ff8822','#22aa22','black'))+
    scale_x_continuous('$q_2$' %>% TeX)+
    scale_y_continuous('$q_1^*$' %>% TeX)+
    theme_bw()

p1
```

Observe que temos um problema muito grave quando $q_2$ se aproxima de $\frac{7}{6}$, pois neste caso $\alpha$ se aproxima de $1$ e $q_1^*$ tende a infinito, ademais, se $q_2>\frac{7}{6}$, temos $\alpha<1$ e $q_1^*<0$! De fato, quando $\alpha<1$, temos que $\mathbb{E}_{\mathcal{NG}}[\lambda_1^2]=\mathbb{E}_{\mathcal{NG}}[\mu^2]=+\infty$, pois a distribuição marginal de $\mu$ é $t$ de Student com $2\alpha$ graus de liberdade e, neste caso, $\mu$ não tem 2º momento finito, logo **não** é possível usar o Teorema da Projeção. De fato, pode ser verificado que **não existe** uma distribuição que minimiza a divergência KL, pois a divergência será uma função descrescente de $q_2$. Há uma forma alternativa de lidar com esta situação que resolve todos estes problemas, porém, antes de apresentar esta proposta, continuaremos com a análise da qualidade do ajuste para os casos onde $\alpha>1$.

Como $q_2$ é a variância de $\lambda_2$, é simples definir casos onde $q_2<\frac{7}{6}$, de fato, o valor de $q_2$ dependerá exclusivamente da estrutura do modelo. Caso a estrutura inclua regressoras para $\phi$, não é possível garantir que $q_2$ não exceda $\frac{7}{6}$ em algum momento, uma vez que $q_2$ dependerá do valor observado para a regressora em questão. Contudo, no caso onde não estamos usando regressoras, fica relativamente fácil controlar o valor de $q_2$, porém, infelizmente, isso significa que não podemos usar prioris muito vagas e/ou estruturas muito complexas para $\phi$. Dito isso, conseguimos fazer um ajuste razoável para casos onde $\phi$ não apresenta tendência de crescimento, mesmo incluindo uma estrutura sazonal para $\phi$. Nos casos onde $\phi$ apresenta tendência de crescimento, é necessário que a priori para o nível do modelo tenha variância baixa. Vale destacar que, em qualquer caso que seja, **não** há restrições sobre a estrutura associada a $\mu$, desta forma, é possível modelar problemas onde o comportamento da média é bem complexo, desde que a variância dos dados **não** precise de muita estrutura.

Adiante, comparamos a distribuição do parâmetro $\mu$ antes e depois da compatibilização das distribuições para vários valores iniciais de $q_2$ (valores válidos). No teste a seguir, começamos com a distribuição $\mathcal{NG}$ (original), obtemos a distribuição Normal associada e depois obtemos a distribuição $\mathcal{NG}$ (aproximada) correspondente. Vale lembrar que, usando a distribuição $\mathcal{NG}$ para $\mu$ e $\phi$, temos que a distribuição marginal de $\mu$ é $t\left(2\alpha,\mu_0,\frac{\beta}{c_0\alpha}\right)$.

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

mu0=0
c0=1
beta=1
for(q2 in seq(0.1,1.1,l=4)){
  helper=-3+3*sqrt(1+2*q2/3)
  alpha=1/helper
  new_norm=do.call(convert_NG_Normal,
                   convert_Normal_NG(list('mu0'=mu0,'c0'=c0,'alpha'=alpha,'beta'=beta)))
  mu0_alt=new_norm$mu0
  c0_alt=new_norm$c0
  alpha_alt=new_norm$alpha
  beta_alt=new_norm$beta
  
  s_or=sqrt((beta/alpha)*(1/c0))
  s_alt=sqrt((beta_alt/alpha_alt)*(1/c0_alt))
  
  
  x=seq(qt(0.025,2*alpha)*s_or+mu0,qt(0.975,2*alpha)*s_or+mu0,l=1000)
  
  fx_true=dt((x-mu0)/s_or,2*alpha)/s_or
  fx_approx=dt((x-mu0_alt)/s_alt,2*alpha_alt)/s_alt
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. aprox.'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(q2,levels=unique(data_plot$q2)),scales ='free',nrow=3))
```

Veja que, quando $q_2$ é próximo de $\frac{7}{6}$ a aproximação é muito ruim. Tanto a distribuição aproximada quanto a original são $t$, mas durante a conversão o valor de $q_1$ é drasticamente inflado, o que faz com que o $c_0$ da aproximação seja muito menor do que o $c_0$ original.

Adiante comparamos a estimação dos parâmetros usando a abordagem KL com a estimação analítica para uma amostra i.i.d. de uma Normal Padrão:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

T=50
mu=0
for(s2 in c(1)){

  set.seed(seed)
  y=rnorm(T,mu,sqrt(s2))
  
  
  level=polynomial_block(order=1,values=1,D=1/1,k=2)
  
  resultado=fit_model(level,
                      outcome = y,
                      family='normal_gamma')
  
  mu0=resultado$conj_prior_param$mu0[1]
  c0=resultado$conj_prior_param$c0[1]
  alpha=resultado$conj_prior_param$alpha[1]
  beta=resultado$conj_prior_param$beta[1]
  
  n=1:T
  obs_y=y
  y_mean=cumsum(obs_y)/n
  y_mean2=cumsum(obs_y**2)/n
  
  s=(cumsum(obs_y**2)-(n*(y_mean)**2))/n
  
  mu0_star <- (c0 * mu0 + n*y_mean) / (c0 + n)
  c0_star <- c0 + n
  alpha_star <- alpha + n/2
  beta_star <- beta + 0.5 * (n*s+((c0*n*(mu0 - y_mean)**2) / (c0 + n)))
  
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             param=mu0_star,
                             label='$\\mu_0$',
                             color='Sol. Analítica'),
                  data.frame(time=1:T,
                             param=c0_star,
                             label='$c_0$',
                             color='Sol. Analítica'),
                  data.frame(time=1:T,
                             param=alpha_star,
                             label='$\\alpha$',
                             color='Sol. Analítica'),
                  data.frame(time=1:T,
                             param=beta_star,
                             label='$\\beta$',
                             color='Sol. Analítica'))
  
    data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,1],
                             label='$\\mu_0$',
                             color='Aproximação KL'),
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,2],
                             label='$c_0$',
                             color='Aproximação KL'),
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,3],
                             label='$\\alpha$',
                             color='Aproximação KL'),
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,4],
                             label='$\\beta$',
                             color='Aproximação KL'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=time,y=param,color=color,linetype=as.character(color == 'Sol. Analítica')))+
  guides(linetype=FALSE)+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#5555ff','black'))+
    scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
    ylab('Parâmetro')+
    theme_bw()+
  facet_wrap(~factor(label,levels=unique(data_plot$label)),scales ='free'))
```

Olhando mais de perto para o parâmetro $\mu_0$, temos a seguinte estivamativa intervalar e pontual para os dois ajustes para uma nova amostra de tamanho maior que a anterior:

```{r}
devtools::load_all()

T=200

mu0=0
for(s2 in c(1)){
set.seed(13031998)
  y=rnorm(T,mu0,sqrt(s2))
  
  
  level=polynomial_block(order=1,values=1,D=1/1,k=2)
  
  resultado=fit_model(level,
                      outcome = y,
                      family='normal_gamma')
  
  mu0=resultado$conj_prior_param$mu0[1]
  c0=resultado$conj_prior_param$c0[1]
  alpha=resultado$conj_prior_param$alpha[1]
  beta=resultado$conj_prior_param$beta[1]
  
  n=1:T
  obs_y=y
  y_mean=cumsum(obs_y)/n
  y_mean2=cumsum(obs_y**2)/n
  
  s=(cumsum(obs_y**2)-(n*(y_mean)**2))/n
  
  mu0_star <- (c0 * mu0 + n*y_mean) / (c0 + n)
  c0_star <- c0 + n
  alpha_star <- alpha + n/2
  beta_star <- beta + 0.5 * (n*s+((c0*n*(mu0 - y_mean)**2) / (c0 + n)))
  
  
  s_or=sqrt((beta_star/alpha_star)*(1/c0_star))
  s_alt=sqrt((resultado$conj_post_param[,4]/resultado$conj_post_param[,3])*(1/resultado$conj_post_param[,2]))

data1=data.frame(time=1:T,
           mean=mu0_star,
           C.I.lower=qt(0.025,2*alpha_star)*s_or+mu0_star,
           C.I.upper=qt(0.975,2*alpha_star)*s_or+mu0_star,
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1],
           C.I.lower=qt(0.025,2*resultado$conj_post_param[,3])*s_alt+resultado$conj_post_param[,1],
           C.I.upper=qt(0.975,2*resultado$conj_post_param[,3])*s_alt+resultado$conj_post_param[,1],
           color='Aproximação KL')

data_plot=rbind(data1,data2)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid'))+
  scale_color_manual('',values=c('#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Filtragem' %>% paste(paste0('($\\sigma^2=',s2,'$)')) %>% TeX)+
  theme_bw()
print(p1)
}
```

Veja que o ajuste dos dados não fica tão ruim (dado os valores no espaço paramétrico, é de se imaginar que o ajuste fosse pior), gerando um resultado que me parece aceitável.

Por último, vamos analisar um caso onde não há solução analítica para verificar se a abordagem KL é capaz de capturar a dinâmica temporal dos dados, em especial da variância observacional. Para tal, geramos uma amostra $y_1,...,y_{200}$, tal que $y_i \sim \mathcal{N}(0,S_i)$ com:

$$
S_i=10\exp\left\{\sin\left(\frac{i\pi}{15}\right)\right\}
$$

```{r}
T <- 200
w <- (200 / 30) * 2 * pi
S=exp((sin(w * 1:T / T)))*10
mu <- 0
set.seed(seed)
outcome <- rnorm(T, mu,sqrt(S))

level <- polynomial_block(
  order = 1,
  values = c(1, 0),
  D = 1 / 1,
  by_time=FALSE
)
variance1 <- polynomial_block(
  order = 1,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)
variance2 <- harmonic_block(
  period = 30,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)

fitted_data <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma")
show_fit(fitted_data, smooth = TRUE,dynamic_plot=FALSE)$plot+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('black'))+
  scale_fill_manual('',values=c('black'))
```

```{r}
p1=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,1],color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  ymax=qt(0.975,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média filtrada')+
  theme_bw()
```

```{r}
p2=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,3]-1),color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  labs(title='Variância filtrada')+
  coord_cartesian(ylim=c(0,100))+
  theme_bw()
```

```{r}
mu0=rep(NA,T)
c0=rep(NA,T)
alpha=rep(NA,T)
beta=rep(NA,T)
for(i in 1:T){
  ft=t(fitted_data$FF[,,i]) %*% fitted_data$mts[,i]
  Qt=t(fitted_data$FF[,,i]) %*% fitted_data$Cts[,,i] %*% fitted_data$FF[,,i]

  mu0[i]=ft[1,]+Qt[1,2]
  c0[i]=exp(-ft[2,] - Qt[2,2] / 2)/Qt[1,1]
  alpha[i]=1/(-3+3*sqrt(1+2*Qt[2,2]/3))
  beta[i]=alpha[i]*exp(-ft[2]-Qt[2,2]/2)
}
```

```{r}
p3=ggplot()+
  geom_line(aes(x=1:T,y=mu0,color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  ymax=qt(0.975,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média suavizada')+
  theme_bw()
```

```{r}
p4=ggplot()+
  geom_line(aes(x=1:T,y=beta/(alpha-1),color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha,beta),
                  ymax=1/qgamma(0.025,alpha,beta),
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância suavizada')+
  coord_cartesian(ylim=c(0,100))+
  theme_bw()
```

```{r fig.height=8}
grid.arrange(p1,p2,p3,p4,ncol=2)
```

As curvas em preto representam o valor real do parâmetro em cada tempo e as curvas em azul representam o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Veja no resultado acima que o modelo consegue capturar bem o comportamento da variância e a dispersão dos dados, provavelmente não tão bem quanto se um método de MCMC fosse utilizado, mas ainda assim o ajuste parece satisfatório.

Adiante mostraremos o ajuste para uma situação parecida, mas agora com um modelo de crescimento linear para a média:

```{r}
T <- 200
w <- (200 / 50) * 2 * pi
S=exp((sin(w * 1:T / T))+1)
mu <- 20*1:T / T
set.seed(13031998)
outcome <- rnorm(T, mu,sqrt(S))

level <- polynomial_block(
  order = 2,
  values=c(1,0),
  D = 1 / 1,
  by_time=FALSE
)
variance1 <- polynomial_block(
  order = 1,
  values=c(0,1),
  D = 1 / 1,
  by_time=FALSE
)
variance2 <- harmonic_block(
  period = 50,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)

fitted_data <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma")
show_fit(fitted_data, smooth = TRUE,dynamic_plot=FALSE)$plot+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('black'))+
  scale_fill_manual('',values=c('black'))
```

```{r}
p1=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,1],color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  ymax=qt(0.975,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média filtrada')+
  theme_bw()
```

```{r}
p2=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,3]-1),color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  labs(title='Variância filtrada')+
  coord_cartesian(ylim=c(0,25))+
  theme_bw()
```

```{r}
mu0=rep(NA,T)
c0=rep(NA,T)
alpha=rep(NA,T)
beta=rep(NA,T)
for(i in 1:T){
  ft=t(fitted_data$FF[,,i]) %*% fitted_data$mts[,i]
  Qt=t(fitted_data$FF[,,i]) %*% fitted_data$Cts[,,i] %*% fitted_data$FF[,,i]

  mu0[i]=ft[1,]+Qt[1,2]
  c0[i]=exp(-ft[2,] - Qt[2,2] / 2)/Qt[1,1]
  alpha[i]=1/(-3+3*sqrt(1+2*Qt[2,2]/3))
  beta[i]=alpha[i]*exp(-ft[2]-Qt[2,2]/2)
}
```

```{r}
p3=ggplot()+
  geom_line(aes(x=1:T,y=mu0,color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  ymax=qt(0.975,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média suavizada')+
  theme_bw()
```

```{r}
p4=ggplot()+
  geom_line(aes(x=1:T,y=beta/(alpha-1),color='Estimação',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha,beta),
                  ymax=1/qgamma(0.025,alpha,beta),
                  color='Estimação',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
        guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#5555ff','black'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância suavizada')+
  coord_cartesian(ylim=c(0,25))+
  theme_bw()
```

```{r fig.height=8}
grid.arrange(p1,p2,p3,p4,ncol=2)
```

As curvas em preto representam o valor real do parâmetro em cada tempo e as curvas em azul representam o ajuste com a abordage KL, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Novamente, as estimativas parecem bem descentes e o modelo parece se adequar bem aos dados observados. De modo geral, acredito que este ajuste seja válido, desde que a modelagem da variância não seja demasiadamente complexa. Em termos prático, uma vez que as condições necessárias para um ajuste aceitável (aceitável na minha perspectiva, claro) estão estabelecidas, é fácil implementar no pacote GDLM restrições sob a modelagem da variância, de modo a impedir que um usuário caia em casos inválidos. De todo modo, adiante apresentarei uma proposta que contorna o problema encontrado na qualidade do ajuste, oferencendo um ajuste mais próximo da solução analítica (nos casos onde podemos calcula-la), válido para quaisquer $q_2$ e $\alpha$ e sem restrições quanto a complexidade da estrutura do modelo, permitindo inclusive o uso de regressoras na modelagem de $\phi$.

O problema que estamos encontrando na compatibilização das distribuições de $\lambda_1, \lambda_2$ e $\mu, \phi$ tem sua origem no fato de que, quando $\alpha\le1$, não exitem $f_1^*, f_2^*, q_1^*, q_2^*, \rho^*$ que minimize a divergência KL na volta (isto é, quando saímos da $\mathcal{NG}$ e vamos para a normal bivariada), de fato, dados $f_1^*, f_2^*, q_1^*, q_2^*, \rho^*$, podemos sempre diminuir a divergência KL ao aumentar o valor de $q_1^*$ e isso se deve ao fato de que, quando $\alpha \le 1$, $\mu$ não tem 2º momento finito.

Pensando um pouco na proposta do artigo, a ideia é que, para a aproximar uma distribuição $p$ pertencente à família exponencial por outra distribuição em um espaço $\Omega$ de distribuições pertencentes à familia exponencial, devemos tomar $q \in \Omega$ tal que $q$ minimize $KL(p||q)$, sendo que a escolha de $\Omega$ é, em certo sentido, arbitrária, sendo usado um conjunto de distribuições com propriedades convenientes. A minha proposta para a resolução do problema é restringir o conjunto $\Omega$ para um conjunto onde a divergência KL possa ser minimizada, i.e., onde existe $q \in \Omega$ que minimize a divergência KL.

No contexto original do caso Normal com variância desconhecida, para irmos da posteriori conjugada para a posteriori normal, usamos (desculpe a notação) $\Omega=\{\mathcal{N}_2(f_1,f_2,q_1,q_2,\rho): f_1,f_2 \in \mathbb{R}, q_1,q_2 \in (0,+\infty), \rho \in (-1,1)\}$. Minha proposta é restringir $\Omega$ ao conjunto das Normais bivariadas com $q_1=c$, de modo que devemos escolher $q \in \Omega'=\{\mathcal{N}_2(f_1,f_2,q_1,q_2,\rho) \in \Omega: q_1=c\}$. Primeiro, afirmo que neste conjunto existe $q$ minimal e que podemos usar o Teorema da Projeção para encontrar $q$, de fato, basta observar que, sob esta restrição, o vetor $H_q$ de estatísticas suficientes será dado por:

$$
H_q=(\lambda_1,\lambda_2,\lambda_2^2,\rho),
$$

sendo que todas estas variáveis tem valor esperado finito **para quaisquer** $\mu_0$, $c_0$, $\alpha$ e $\beta$, de modo que podemos encontrar $q$ resolvendo o seguinte sistema (pois as distribuições em $\Omega'$ ainda pertencem à família exponencial):

\begin{equation}
\label{sis:NG5}
\begin{aligned}
\mathbb{E}_\mathcal{N}[H_q]=\mathbb{E}_\mathcal{NG}[H_q],
\end{aligned}
\end{equation}

Resolvendo o sistema acima obtemos os seguintes parâmetros para a posteriori Normal:

\begin{equation}
\label{sis:NG6}
\begin{aligned}
  f_1^{*}  =& \mu^{*}_0\\
  f_2^{*}   =& \psi(\alpha^{*})-\ln(\beta^{*})\\
  q_2^{*}  =& \psi'(\alpha^{*})\\
  \rho^{*} =& 0,
\end{aligned}
\end{equation}

ademais, como $q \in \Omega'$, temos que $q_1=c$.

Para completar esta proposta, resta apenas escolher o valor $c$. Acredito que a escolha ideal (e mais natural) seria $c= \frac{\beta}{c_0\alpha}$, pois:

- $\frac{\beta}{c_0\alpha} \approx \frac{\beta}{c_0(\alpha-1)}$ para $\alpha$ grande, isto é, o mínimo em $\Omega'$ se aproxima do mínimo global da divergência KL quando esse mínimo existe, assim, não há perdas significativas na qualidade da aproximação quando $\alpha$ é grande;
- $\frac{\beta}{c_0\alpha}$ é bem comportado para qualquer possível valor de $\alpha$. De fato, na proposta original, mesmo quando $\alpha>1$, se $\alpha$ fosse próximo de $1$ o ajuste ficava numericamente instável, prejudicando a qualidade da aproximação mesmo em valores válidos de $\alpha$. Naturalmente, nesta nova proposta, ainda temos problemas se $\alpha \approx 0$, contudo, ao atualizarmos a nossa priori e obtermos nossa posteriori, é garantido que $\alpha \ge 0.5$, logo é garantido que a aproximação será sempre bem comportada numericamente;
- $\frac{\beta}{c_0\alpha}$ garante que não há distorção na compatibilização das distribuições, isto é, se começamos com $\lambda_1,\lambda_2 \sim \mathcal{N}_2(f_1,q_1,f_2,q_2,\rho)$, obtemos a priori conjugada $\mathcal{NG}$ e retornamos para a priori Normal sem atualizar os parâmetros da $\mathcal{NG}$, é garantido que a variância na volta será idêntica a da ida;
- $\frac{\beta}{c_0\alpha}$ é o parâmetro de escala da distribuição marginal de $\mu$, de fato, se $\mu,\phi \sim \mathcal{NG}(\mu_0,c_0,\alpha, \beta)$, então $\mu \sim t\left(2\alpha,\mu_0,\frac{\beta}{c_0\alpha}\right)$, logo, $\frac{\mu-\mu_0}{\sqrt{\frac{\beta}{c_0\alpha}}} \sim t\left(2\alpha\right)$. Assim, em essência, a minha proposta equivale a aproximar a distruibuição de $\frac{\mu-\mu_0}{\sqrt{\frac{\beta}{c_0\alpha}}}$ por uma Normal Padrão.

Por último, para completar o argumento, apresentarei a qualidade da aproximação usando a nova proposta em testes análogos aos realizados com a proposta original.

Comparação da distribuição marginal de $\mu$ original e após a compatibilização:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

mu0=0
c0=1
beta=1
for(q2 in c(seq(0.1,1.1,l=4))){
  helper=-3+3*sqrt(1+2*q2/3)
  alpha=1/helper
  new_norm=do.call(convert_NG_Normal,
                   convert_Normal_NG(list('mu0'=mu0,'c0'=c0,'alpha'=alpha,'beta'=beta)))
  mu0_alt1=new_norm$mu0
  c0_alt1=new_norm$c0
  alpha_alt1=new_norm$alpha
  beta_alt1=new_norm$beta
  
  new_norm=do.call(convert_NG_Normal,
                   convert_Normal_NG_cor(list('mu0'=mu0,'c0'=c0,'alpha'=alpha,'beta'=beta)))
  mu0_alt2=new_norm$mu0
  c0_alt2=new_norm$c0
  alpha_alt2=new_norm$alpha
  beta_alt2=new_norm$beta
  
  s_or=sqrt((beta/alpha)*(1/c0))
  s_alt1=sqrt((beta_alt1/alpha_alt1)*(1/c0_alt1))
  s_alt2=sqrt((beta_alt2/alpha_alt2)*(1/c0_alt2))
  
  
  x=seq(qt(0.025,2*alpha)*s_or+mu0,qt(0.975,2*alpha)*s_or+mu0,l=1000)
  
  fx_true=dt((x-mu0)/s_or,2*alpha)/s_or
  fx_approx1=dt((x-mu0_alt1)/s_alt1,2*alpha_alt1)/s_alt1
  fx_approx2=dt((x-mu0_alt2)/s_alt2,2*alpha_alt2)/s_alt2
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx1,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. aprox.\nproposta original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx2,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. aprox.\nnova proposta'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(q2,levels=unique(data_plot$q2)),scales ='free',nrow=3))
```

Veja que há uma melhoria drástica na qualidade da aproximação com a nova proposta, ademais, com a proposta antiga, não é possível fazer a aproximação no casos em que $q_2\ge \frac{7}{6}$, em contrapartida, na nova proposta, podemos fazer a aproximação para qualquer valor de $q_2$:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

mu0=0
c0=1
beta=1
for(q2 in c(7/6,1.5,2,3,5,10)){
  helper=-3+3*sqrt(1+2*q2/3)
  alpha=1/helper

  new_norm=do.call(convert_NG_Normal,
                   convert_Normal_NG_cor(list('mu0'=mu0,'c0'=c0,'alpha'=alpha,'beta'=beta)))
  mu0_alt2=new_norm$mu0
  c0_alt2=new_norm$c0
  alpha_alt2=new_norm$alpha
  beta_alt2=new_norm$beta
  
  s_or=sqrt((beta/alpha)*(1/c0))
  s_alt2=sqrt((beta_alt2/alpha_alt2)*(1/c0_alt2))
  
  
  x=seq(qt(0.025,2*alpha)*s_or+mu0,qt(0.975,2*alpha)*s_or+mu0,l=1000)
  
  fx_true=dt((x-mu0)/s_or,2*alpha)/s_or
  fx_approx2=dt((x-mu0_alt2)/s_alt2,2*alpha_alt2)/s_alt2
  
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_true,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. original'))
  data_plot=rbind(data_plot,
                  data.frame(x=x,
                             fx=fx_approx2,
                             q2=paste0('$\\q2=',q2 %>% round(2),'$'),
                             color='Distr. aprox.\nnova proposta'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=x,y=fx,color=color,linetype=as.character(color == 'Distr. original')))+
  guides(linetype=FALSE)+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#22aa22','black'))+
    scale_x_continuous('$x$' %>% TeX,expand=c(0,0))+
    scale_y_continuous('$f(x)$' %>% TeX)+
    ylab('$f(x)$' %>% TeX)+
    theme_bw()+
  facet_wrap(~factor(q2,levels=unique(data_plot$q2)),scales ='free',nrow=3))
```

Podemos observar que a nova proposta se mostra robusta mesmo para valores bem elevados de $q_2$.

Adiante comparamos a estimação dos parâmetros usando a abordagem KL com a estimação analítica para uma amostra i.i.d. de uma Normal Padrão:

```{r}
data_plot=data.frame(time=NULL,alpha=NULL,lambda=NULL,color=NULL)

T=50
mu=0
for(s2 in c(1)){

  set.seed(seed)
  y=rnorm(T,mu,sqrt(s2))
  
  
  level=polynomial_block(order=1,values=1,D=1/1,k=2)
  
  resultado=fit_model(level,
                      outcome = y,
                      family='normal_gamma')
  
  mu0=resultado$conj_prior_param$mu0[1]
  c0=resultado$conj_prior_param$c0[1]
  alpha=resultado$conj_prior_param$alpha[1]
  beta=resultado$conj_prior_param$beta[1]
  
  resultado2=fit_model(level,
                      outcome = y,
                      family='normal_gamma_cor')
  
  n=1:T
  obs_y=y
  y_mean=cumsum(obs_y)/n
  y_mean2=cumsum(obs_y**2)/n
  
  s=(cumsum(obs_y**2)-(n*(y_mean)**2))/n
  
  mu0_star <- (c0 * mu0 + n*y_mean) / (c0 + n)
  c0_star <- c0 + n
  alpha_star <- alpha + n/2
  beta_star <- beta + 0.5 * (n*s+((c0*n*(mu0 - y_mean)**2) / (c0 + n)))
  
  data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             param=mu0_star,
                             label='$\\mu_0$',
                             color='Sol. Analítica'),
                  data.frame(time=1:T,
                             param=c0_star,
                             label='$c_0$',
                             color='Sol. Analítica'),
                  data.frame(time=1:T,
                             param=alpha_star,
                             label='$\\alpha$',
                             color='Sol. Analítica'),
                  data.frame(time=1:T,
                             param=beta_star,
                             label='$\\beta$',
                             color='Sol. Analítica'))
  
    data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,1],
                             label='$\\mu_0$',
                             color='Aproximação KL original'),
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,2],
                             label='$c_0$',
                             color='Aproximação KL original'),
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,3],
                             label='$\\alpha$',
                             color='Aproximação KL original'),
                  data.frame(time=1:T,
                             param=resultado$conj_post_param[,4],
                             label='$\\beta$',
                             color='Aproximação KL original'))
    
    data_plot=rbind(data_plot,
                  data.frame(time=1:T,
                             param=resultado2$conj_post_param[,1],
                             label='$\\mu_0$',
                             color='Aproximação KL nova'),
                  data.frame(time=1:T,
                             param=resultado2$conj_post_param[,2],
                             label='$c_0$',
                             color='Aproximação KL nova'),
                  data.frame(time=1:T,
                             param=resultado2$conj_post_param[,3],
                             label='$\\alpha$',
                             color='Aproximação KL nova'),
                  data.frame(time=1:T,
                             param=resultado2$conj_post_param[,4],
                             label='$\\beta$',
                             color='Aproximação KL nova'))
}
```


```{r}
(ggplot(data_plot)+
    geom_line(aes(x=time,y=param,color=color,linetype=as.character(color == 'Sol. Analítica')))+
  guides(linetype=FALSE)+
    geom_vline(xintercept = 0,linetype='solid',color='#555555')+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
    scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
    scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
    ylab('Parâmetro')+
    theme_bw()+
  facet_wrap(~factor(label,levels=unique(data_plot$label)),scales ='free'))
```

Observe que não há melhoria significativa na estimação do $\alpha$ e do $\beta$ com a nova proposta, contudo, há um ganho significativo na estimação de $\mu_0$ e $c_0$. Vale destacar que a qualidade da aproximação da distribuição da variância será a mesma da observada no caso Gamma com parâmetro de forma igual a 0.5, assim, não há muito o que ser feito sobre os parâmetros $\alpha$ e $\beta$.

Olhando mais de perto para o parâmetro $\mu$, temos a seguinte estivamativa intervalar e pontual para os três ajustes para uma nova amostra de tamanho maior que a anterior:

```{r}
devtools::load_all()

T=200

mu0=0
for(s2 in c(1)){
set.seed(13031998)
  y=rnorm(T,mu0,sqrt(s2))
  
  
  level=polynomial_block(order=1,values=1,D=1/1,k=2)
  
  resultado=fit_model(level,
                      outcome = y,
                      family='normal_gamma')
  resultado2=fit_model(level,
                      outcome = y,
                      family='normal_gamma_cor')
  
  mu0=resultado$conj_prior_param$mu0[1]
  c0=resultado$conj_prior_param$c0[1]
  alpha=resultado$conj_prior_param$alpha[1]
  beta=resultado$conj_prior_param$beta[1]
  
  n=1:T
  obs_y=y
  y_mean=cumsum(obs_y)/n
  y_mean2=cumsum(obs_y**2)/n
  
  s=(cumsum(obs_y**2)-(n*(y_mean)**2))/n
  
  mu0_star <- (c0 * mu0 + n*y_mean) / (c0 + n)
  c0_star <- c0 + n
  alpha_star <- alpha + n/2
  beta_star <- beta + 0.5 * (n*s+((c0*n*(mu0 - y_mean)**2) / (c0 + n)))
  
  
  s_or=sqrt((beta_star/alpha_star)*(1/c0_star))
  s_alt=sqrt((resultado$conj_post_param[,4]/resultado$conj_post_param[,3])*(1/resultado$conj_post_param[,2]))
  s_alt2=sqrt((resultado2$conj_post_param[,4]/resultado2$conj_post_param[,3])*(1/resultado2$conj_post_param[,2]))

data1=data.frame(time=1:T,
           mean=mu0_star,
           C.I.lower=qt(0.025,2*alpha_star)*s_or+mu0_star,
           C.I.upper=qt(0.975,2*alpha_star)*s_or+mu0_star,
           color='Sol. analítica')

data2=data.frame(time=1:T,
           mean=resultado$conj_post_param[,1],
           C.I.lower=qt(0.025,2*resultado$conj_post_param[,3])*s_alt+resultado$conj_post_param[,1],
           C.I.upper=qt(0.975,2*resultado$conj_post_param[,3])*s_alt+resultado$conj_post_param[,1],
           color='Aproximação KL original')

data3=data.frame(time=1:T,
           mean=resultado2$conj_post_param[,1],
           C.I.lower=qt(0.025,2*resultado2$conj_post_param[,3])*s_alt2+resultado2$conj_post_param[,1],
           C.I.upper=qt(0.975,2*resultado2$conj_post_param[,3])*s_alt2+resultado2$conj_post_param[,1],
           color='Aproximação KL nova')

data_plot=rbind(data1,data2,data3)

p1=ggplot(data_plot)+
  geom_line(aes(x=time,y=mean,color=color,linetype='Média'))+
  geom_ribbon(aes(x=time,ymin=C.I.lower,ymax=C.I.upper,color=color,linetype='I.C. 95 %'),alpha=0)+
  scale_x_continuous('$t$' %>% TeX,expand=c(0,0))+
  scale_y_continuous('$\\alpha$' %>% TeX)+
  scale_linetype_manual('',values=c('dashed','solid'))+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  labs(title='Filtragem' %>% paste(paste0('($\\sigma^2=',s2,'$)')) %>% TeX)+
  theme_bw()
print(p1)
}
```

Novamente, temos que a qualidade do ajuste da média é muito superior na nova abordagem.

Em seguida vamos avaliar alguns casos onde não há solução analítica (os mesmo casos avaliados anteriormente):

```{r}
T <- 200
w <- (200 / 30) * 2 * pi
S=exp((sin(w * 1:T / T)))*10
mu <- 0
set.seed(seed)
outcome <- rnorm(T, mu,sqrt(S))

level <- polynomial_block(
  order = 1,
  values = c(1, 0),
  D = 1 / 1,
  by_time=FALSE
)
variance1 <- polynomial_block(
  order = 1,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)
variance2 <- harmonic_block(
  period = 30,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)

fitted_data <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma")
fitted_data2 <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma_cor")
data_plot1=show_fit(fitted_data, smooth = TRUE,dynamic_plot=FALSE)$data
data_plot2=show_fit(fitted_data2, smooth = TRUE,dynamic_plot=FALSE)$data
```

```{r}
ggplot()+
  geom_point(aes(x=1:T,y=outcome,color='Valores observados'))+
  geom_line(aes(x=1:T,y=data_plot1$Prediction,color='Aprox. KL original'))+
  geom_ribbon(aes(x=1:T,
                  ymin=data_plot1$C.I.lower,
                  ymax=data_plot1$C.I.upper,
                  color='Aprox. KL original'),alpha=0,linetype='dashed')+
  geom_line(aes(x=1:T,y=data_plot2$Prediction,color='Aprox. KL nova'))+
  geom_ribbon(aes(x=1:T,
                  ymin=data_plot2$C.I.lower,
                  ymax=data_plot2$C.I.upper,
                  color='Aprox. KL nova'),alpha=0,linetype='dashed')+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  labs(title='Ajuste dos dados')+
  theme_bw()
```

Veja que os dois modelos capturam bem o comportamento dos dados, sendo que a principal diferença entre a proposta original e a nova proposta é a estimativa da média que, na nova proposta, está mais próxima do valor correto (0). Vale destacar que a diferença entre os i.c. é essencialmente uma questão de locação, pois, como verêmos adiante, as duas abordagens tem uma estimativas bem próxima para a variância:

```{r}
p1=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,3]-1),color='Estimação KL original',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  color='Estimação KL original',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=fitted_data2$conj_post_param[,4]/(fitted_data2$conj_post_param[,3]-1),color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data2$conj_post_param[,3],fitted_data2$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data2$conj_post_param[,3],fitted_data2$conj_post_param[,4]),
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância filtrada')+
  coord_cartesian(ylim=c(0,100))+
  theme_bw()
```

```{r}
alpha=rep(NA,T)
beta=rep(NA,T)
alpha2=rep(NA,T)
beta2=rep(NA,T)
for(i in 1:T){
  ft=t(fitted_data$FF[,,i]) %*% fitted_data$mts[,i]
  Qt=t(fitted_data$FF[,,i]) %*% fitted_data$Cts[,,i] %*% fitted_data$FF[,,i]
  
  ft2=t(fitted_data2$FF[,,i]) %*% fitted_data2$mts[,i]
  Qt2=t(fitted_data2$FF[,,i]) %*% fitted_data2$Cts[,,i] %*% fitted_data2$FF[,,i]

  alpha[i]=1/(-3+3*sqrt(1+2*Qt[2,2]/3))
  beta[i]=alpha[i]*exp(-ft[2]-Qt[2,2]/2)
  alpha2[i]=1/(-3+3*sqrt(1+2*Qt2[2,2]/3))
  beta2[i]=alpha2[i]*exp(-ft2[2]-Qt2[2,2]/2)
}
```


```{r}
p2=ggplot()+
  geom_line(aes(x=1:T,y=beta/(alpha-1),color='Estimação KL original',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha,beta),
                  ymax=1/qgamma(0.025,alpha,beta),
                  color='Estimação KL original',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=beta2/(alpha2-1),color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha2,beta2),
                  ymax=1/qgamma(0.025,alpha2,beta2),
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância suavizada')+
  coord_cartesian(ylim=c(0,100))+
  theme_bw()
```

```{r fig.height=4}
grid.arrange(p1,p2,ncol=2)
```

As curvas em preto representam o valor real do parâmetro em cada tempo, as curvas em azul representam o ajuste com a abordage KL e as curvas em verde representam o ajuste com a nova proposta, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Adiante mostraremos o ajuste para uma situação parecida, mas agora com um modelo de crescimento linear para a média:

```{r}
T <- 200
w <- (200 / 50) * 2 * pi
S=exp((sin(w * 1:T / T))+1)
mu <- 20*1:T / T
set.seed(13031998)
outcome <- rnorm(T, mu,sqrt(S))

level <- polynomial_block(
  order = 2,
  values = c(1, 0),
  D = 1 / 1,
  by_time=FALSE
)
variance1 <- polynomial_block(
  order = 1,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)
variance2 <- harmonic_block(
  period = 50,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)

fitted_data <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma")
fitted_data2 <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma_cor")
data_plot1=show_fit(fitted_data, smooth = TRUE,dynamic_plot=FALSE)$data
data_plot2=show_fit(fitted_data2, smooth = TRUE,dynamic_plot=FALSE)$data
```

```{r}
ggplot()+
  geom_point(aes(x=1:T,y=data_plot1$Observation,color='Valores observados'))+
  geom_line(aes(x=1:T,y=data_plot1$Prediction,color='Aprox. KL original'))+
  geom_ribbon(aes(x=1:T,
                  ymin=data_plot1$C.I.lower,
                  ymax=data_plot1$C.I.upper,
                  color='Aprox. KL original'),alpha=0,linetype='dashed')+
  geom_line(aes(x=1:T,y=data_plot2$Prediction,color='Aprox. KL nova'))+
  geom_ribbon(aes(x=1:T,
                  ymin=data_plot2$C.I.lower,
                  ymax=data_plot2$C.I.upper,
                  color='Aprox. KL nova'),alpha=0,linetype='dashed')+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  labs(title='Ajuste dos dados')+
  theme_bw()
```

```{r}
p1=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,1],color='Estimação KL original',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  ymax=qt(0.975,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  color='Estimação KL original',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=fitted_data2$conj_post_param[,1],color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*fitted_data2$conj_post_param[,3])*sqrt(fitted_data2$conj_post_param[,4]/(fitted_data2$conj_post_param[,2]*(fitted_data2$conj_post_param[,3]-1)))+fitted_data2$conj_post_param[,1],
                  ymax=qt(0.975,2*fitted_data2$conj_post_param[,3])*sqrt(fitted_data2$conj_post_param[,4]/(fitted_data2$conj_post_param[,2]*(fitted_data2$conj_post_param[,3]-1)))+fitted_data2$conj_post_param[,1],
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média filtrada')+
  theme_bw()
```


```{r}
p2=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,3]-1),color='Estimação KL original',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  color='Estimação KL original',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=fitted_data2$conj_post_param[,4]/(fitted_data2$conj_post_param[,3]-1),color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data2$conj_post_param[,3],fitted_data2$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data2$conj_post_param[,3],fitted_data2$conj_post_param[,4]),
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância filtrada')+
  coord_cartesian(ylim=c(0,10))+
  theme_bw()
```

```{r}
mu0=rep(NA,T)
c0=rep(NA,T)
alpha=rep(NA,T)
beta=rep(NA,T)
mu02=rep(NA,T)
c02=rep(NA,T)
alpha2=rep(NA,T)
beta2=rep(NA,T)
for(i in 1:T){
  ft=t(fitted_data$FF[,,i]) %*% fitted_data$mts[,i]
  Qt=t(fitted_data$FF[,,i]) %*% fitted_data$Cts[,,i] %*% fitted_data$FF[,,i]
  
  ft2=t(fitted_data2$FF[,,i]) %*% fitted_data2$mts[,i]
  Qt2=t(fitted_data2$FF[,,i]) %*% fitted_data2$Cts[,,i] %*% fitted_data2$FF[,,i]

  mu0[i]=ft[1,]+Qt[1,2]
  c0[i]=exp(-ft[2,] - Qt[2,2] / 2)/Qt[1,1]
  alpha[i]=1/(-3+3*sqrt(1+2*Qt[2,2]/3))
  beta[i]=alpha[i]*exp(-ft[2]-Qt[2,2]/2)
  
  mu02[i]=ft2[1,]+Qt2[1,2]
  c02[i]=exp(-ft2[2,] - Qt2[2,2] / 2)/Qt2[1,1]
  alpha2[i]=1/(-3+3*sqrt(1+2*Qt2[2,2]/3))
  beta2[i]=alpha2[i]*exp(-ft2[2]-Qt2[2,2]/2)
}
```


```{r}
p3=ggplot()+
  geom_line(aes(x=1:T,y=mu0,color='Estimação KL original',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  ymax=qt(0.975,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  color='Estimação KL original',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu02,color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*alpha2)*sqrt(beta2/(c02*(alpha2-1)))+mu02,
                  ymax=qt(0.975,2*alpha2)*sqrt(beta2/(c02*(alpha2-1)))+mu02,
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média suavizada')+
  theme_bw()
```


```{r}
p4=ggplot()+
  geom_line(aes(x=1:T,y=beta/(alpha-1),color='Estimação KL original',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha,beta),
                  ymax=1/qgamma(0.025,alpha,beta),
                  color='Estimação KL original',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=beta2/(alpha2-1),color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha2,beta2),
                  ymax=1/qgamma(0.025,alpha2,beta2),
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
  scale_color_manual('',values=c('#22aa22','#5555ff','black'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância filtrada')+
  coord_cartesian(ylim=c(0,10))+
  theme_bw()
```

```{r}
grid.arrange(p1,p2,p3,p4,ncol=2)
```

As curvas em preto representam o valor real do parâmetro em cada tempo, as curvas em azul representam o ajuste com a abordage KL original e as curvas em verde representam o ajuste com a nova proposta, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Novamente, observamos uma melhoria significativa na estimação de $\mu$ com a nova abordagem, sendo que as estimativas de $\phi$ tem praticamente a mesma qualidade.

Finalmente, o último exemplo a ser mostrado é um caso onde a proposta original não pode ser utilizada, i.e., um caso onde o comportamento da variância exige um modelo com estrutura mais complexa, fazendo com que o valor inicial de $q_2$ seja grande e a proposta original se torne inválida. Ajustaremos o modelo para dados gerado de uma Normal com média $\mu_i$ e variância $S_i$ tais que:


$$
\begin{aligned}
  \mu_i=& \frac{i}{10} \\
  S_i=&\exp\left\{2\sin\left(\frac{i\pi}{25}\right)-20\left(\frac{i}{20}\right)\left(\frac{i}{200}-1\right)-1\right\}\\
\end{aligned}
$$ 

```{r}

devtools::load_all()
T <- 200
w <- (200 / 50) * 2 * pi
S=exp(2*(sin(w * 1:T / T)))*exp(-20*(1:T/T)*((1:T/T)-1))/10
#S=exp(-10*(1:T/T)*((1:T/T)-1))
mu <- 20*1:T / T
set.seed(13031998)
outcome <- rnorm(T, mu,sqrt(S))

level <- polynomial_block(
  order = 2,
  values=c(1,0),
  D = 1 / 1,
  by_time=FALSE
)
variance1 <- polynomial_block(
  order = 3,
  values=c(0,1),
  D = 1 / 1,
  C0= diag(c(1,0.1,0.01)),
  by_time=FALSE
)
variance2 <- harmonic_block(
  period = 50,
  values = c(0, 1),
  D = 1 / 1,
  by_time=FALSE
)

fitted_data <- fit_model(level, variance1, variance2, outcome = outcome, family = "normal_gamma_cor")
show_fit(fitted_data, smooth = TRUE,dynamic_plot=FALSE)$plot+
    geom_hline(yintercept = 0,linetype='solid',color='#555555')+
  scale_color_manual('',values=c('black'))+
  scale_fill_manual('',values=c('black'))
```

```{r}
p1=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,1],color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  ymax=qt(0.975,2*fitted_data$conj_post_param[,3])*sqrt(fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,2]*(fitted_data$conj_post_param[,3]-1)))+fitted_data$conj_post_param[,1],
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média filtrada')+
  theme_bw()
```

```{r}
p2=ggplot()+
  geom_line(aes(x=1:T,y=fitted_data$conj_post_param[,4]/(fitted_data$conj_post_param[,3]-1),color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  ymax=1/qgamma(0.025,fitted_data$conj_post_param[,3],fitted_data$conj_post_param[,4]),
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#22aa22','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância filtrada')+
  coord_cartesian(ylim=c(0,150))+
  theme_bw()
```

```{r}
mu0=rep(NA,T)
c0=rep(NA,T)
alpha=rep(NA,T)
beta=rep(NA,T)
for(i in 1:T){
  ft=t(fitted_data$FF[,,i]) %*% fitted_data$mts[,i]
  Qt=t(fitted_data$FF[,,i]) %*% fitted_data$Cts[,,i] %*% fitted_data$FF[,,i]

  mu0[i]=ft[1,]+Qt[1,2]
  c0[i]=exp(-ft[2,] - Qt[2,2] / 2)/Qt[1,1]
  alpha[i]=1/(-3+3*sqrt(1+2*Qt[2,2]/3))
  beta[i]=alpha[i]*exp(-ft[2]-Qt[2,2]/2)
}
```

```{r}
p3=ggplot()+
  geom_line(aes(x=1:T,y=mu0,color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=qt(0.025,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  ymax=qt(0.975,2*alpha)*sqrt(beta/(c0*(alpha-1)))+mu0,
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=mu,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\mu$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#aaddaa','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Média suavizada')+
  theme_bw()
```

```{r}
p4=ggplot()+
  geom_line(aes(x=1:T,y=beta/(alpha-1),color='Estimação KL nova',
                  linetype='Estimação média'))+
  geom_ribbon(aes(x=1:T,
                  ymin=1/qgamma(0.975,alpha,beta),
                  ymax=1/qgamma(0.025,alpha,beta),
                  color='Estimação KL nova',
                  linetype='Estimação I.C.'),alpha=0)+
  geom_line(aes(x=1:T,y=S,color='Valor real',linetype='Valor real'))+
  scale_y_continuous('$\\sigma^2$' %>% TeX)+
  guides(color=FALSE,linetype=FALSE)+
  scale_color_manual('',values=c('#aaddaa','black'))+
  scale_linetype_manual('',values=c('dashed','solid','solid'))+
  labs(title='Variância suavizada')+
  theme_bw()
```

```{r}
grid.arrange(p1,p2,p3,p4,ncol=2)
```

As curvas em preto representam o valor real do parâmetro em cada tempo e as curvas em verde representam o ajuste com a nova proposta, ademais, as curvas pontilhadas representam o i.c. de 95% e as sólidas representam as estimativas pontuais a cada tempo.

Concluimos então que a nova proposta não só produz uma aproximação superior para a média, mas também permite que a modelagem de $\phi$ seja feita sem restrições quanto a complexiadade.
